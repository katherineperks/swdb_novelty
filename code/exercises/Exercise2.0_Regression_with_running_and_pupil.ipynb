{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1a8ea9",
   "metadata": {},
   "source": [
    "![cropped-SummerWorkshop_Header.png](../../resources/cropped-SummerWorkshop_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e4bcd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h2 align=\"center\"> Using regression to quantify coding of running speed and pupil diameter </h2> \n",
    "<h3 align=\"center\"> SWDB 2024 - Day 2 - Single cell coding</h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db894b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Behavior, attention, and learning shape sensory coding </h3>\n",
    "\n",
    "<p> Many studies over the last decade have shown that animal behavior, attention, and learning can influence neural coding even in sensory areas, often in a cell type specific way. Here are a few examples: \n",
    "\n",
    "* Animal movements, including locomotion, can alter the gain of sensory responses in the visual cortex **(Niell 2011; Darladat 2017)** and aross the brain **(Steinmetz 2019)**. \n",
    "* The influence of movement on sensory coding can be cell type specific **(DiPoppa, 2018)** and different populations of inhibitory neurons are jointly modulated by behavior and sensory features in distinct ways **(Millman 2020)**.\n",
    "* Attention and arousal (alertness), as indexed by pupil diameter, are associated with enhanced sensory coding through changes in neural correlations among specific cell types **(Poort 2022)**.\n",
    "* Sensory tuning can also be shaped by learning to associate a stimulus with a reward **(Shuler 2006, panel b)**, sharpening tuning and enhancing response reliability **(Poort 2018; Khan 2020)**. \n",
    "* Repeated experience with a sensory stimulus can lead to changes in neural representations, including developing predictive activity and expectation signals **(Fiser 2016)**\n",
    "* Novelty, surprise, and stimulus omission also reveal cell type specific differences in coding properties **(Garrett 2019, 2023)**.\n",
    "\n",
    "Here is a brief review of these findings: https://doi.org/10.1016/j.conb.2018.04.020 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527e2df",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> A key question is - How do individual neurons represent information about locomotion, arousal, and behavioral state?</h3>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22772e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Coding for running speed and pupil diameter </h4>\n",
    "<p> \n",
    "\n",
    "Both the Visual Coding and Visual Behavior datasets include the animal's **running speed and pupil diameter** in addition to neural measurements. \n",
    "\n",
    "In the **Visual Behavior** dataset, the mouse is actively performing a task and attending to the visual stimulus. The animal's running speed is unrelated to the task and mice can choose to run or stop as they please. Interestingly, examination of running behavior during task performance indicates that mice often slow down and stop when they respond to a change stimulus, and also appear to modulate their running in anticipation of stimulus onset. In addition, there are interesting pupil dynamics that occur during the task, and in the spontaneous activity periods before and after the task. \n",
    "\n",
    "In this notebook, we will ask questions about single cell coding of behavior features using the **Visual Behavior** dataset. An interesting extension would be to compare to the **Visual Coding** dataset, or to **passive** sessions within the Visual Behavior dataset, to see if active task performance influences the coupling of cell activity with behavior variables compared to when mice passively view visual stimuli. \n",
    "\n",
    "You can see an example of mouse behavior during the **change detection task** in the **Visual Behavior** dataset in the plot below. The blue bars indicate the time of image changes, and the gray bars represent repeated image presentations. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c63ec",
   "metadata": {},
   "source": [
    "![behavior_timeseries_color.png](../../resources/behavior_timeseries_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda383f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "In this exercise, first we will load a dataset and examine `running_speed` and `pupil_width`. \n",
    "\n",
    "Then we will use the `brain_observatory_utilities` to align neural activity and behavior to stimulus onset, and compute the mean value of each measure during each stimulus presentation. \n",
    "\n",
    "We will use the average cell activity, running, and pupil for each `stimulus_presentation` as the input to our Linear Regression model, and examine the contribution of each feature to single cell activity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb469bc",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Data access - loading an experiment of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import these modules to get started\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn makes pretty plots & sets font sizes nicely\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'lines.markeredgewidth': 2})\n",
    "\n",
    "# magic functions for jupyter notebook plotting\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2538f5",
   "metadata": {
    "papermill": {
     "duration": 0.044697,
     "end_time": "2023-07-31T19:17:31.563323",
     "exception": false,
     "start_time": "2023-07-31T19:17:31.518626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confirm that you are currently using the newest version of SDK (2.16.2)\n",
    "import allensdk\n",
    "allensdk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070402b",
   "metadata": {},
   "source": [
    "#### Use the AllenSDK's `VisualBehaviorOphysProjectCache` class to load the `ophys_experiment_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fce480",
   "metadata": {
    "papermill": {
     "duration": 0.052759,
     "end_time": "2023-07-31T19:17:37.505024",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.452265",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if 'Darwin' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2024/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on CodeOcean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import behavior projet cache class from SDK to be able to load the data\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache\n",
    "\n",
    "cache = VisualBehaviorOphysProjectCache.from_local_cache(cache_dir=data_root, use_static_cache=True)\n",
    "# if we needed to download the data we could have used the following line\n",
    "# cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata table for all ophys experiments\n",
    "ophys_experiment_table = cache.get_ophys_experiment_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4ef51",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get an experiment and load the data from the cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33363898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our experiment from yesterday that had interesting looking activity\n",
    "ophys_experiment_id = 951980471\n",
    "\n",
    "# check the metadata to see what conditions it was imaged in\n",
    "ophys_experiment_table.loc[ophys_experiment_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using the cache\n",
    "ophys_experiment = cache.get_behavior_ophys_experiment(ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75857e8",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Data streams: running, pupil, neural activity, and stimulus presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bc8a2",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Examining behavior variables </h4>\n",
    "\n",
    "Let's start by looking at running speed and pupil diameter and assessing their relationships to each other and to neural activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24210efd",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `running_speed` attribute of the `ophys_experiment` data object and plot the running speed. \n",
    "\n",
    "Remember that running speed is sampled at the stimulus display frequency, so you can use `stimulus_timestamps` to plot time on the x-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the running speed, with stimulus_timestamps on x-axis\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (20,5))\n",
    "# FILL IN THIS LINE TO PLOT RUNNING SPEED\n",
    "ax.set_xlabel('Time in session (seconds)')\n",
    "ax.set_ylabel('Running speed (cm/s)')\n",
    "ax.set_title('Ophys experiment ID {}'.format(ophys_experiment_id), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d210e97",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `eye_tracking` attribute of the dataset object. What are the columns? \n",
    "\n",
    "Plot `pupil_area` over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats in the eye_tracking table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2595444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pupil area over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b0e3a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the `running_speed` and `pupil_area` on the same axies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf352a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb18717f",
   "metadata": {},
   "source": [
    "Ok now lets get the neural activity traces and timing of stimuli during the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd6d86",
   "metadata": {},
   "source": [
    "#### Get dF/F traces and events, along with stimulus presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get all the data we need\n",
    "\n",
    "# Get normalized fluorescence traces\n",
    "dff_traces = ophys_experiment.dff_traces.copy()\n",
    "\n",
    "# Get deconvolved events\n",
    "events = ophys_experiment.events.copy()\n",
    "\n",
    "# Get timestamps \n",
    "ophys_timestamps = ophys_experiment.ophys_timestamps.copy()\n",
    "\n",
    "# Get stimulus presentations\n",
    "stimulus_presentations = ophys_experiment.stimulus_presentations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc710a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot one cell's dF/F trace, with time in seconds on the x-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a47fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7091938e",
   "metadata": {},
   "source": [
    "How can we relate cell activity to running speed or pupil diameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826072d",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Check the `metadata` attribute of the ophys experiment dataset object. \n",
    "\n",
    "What is the `ophys_frame_rate`? What is the `stimulus_frame_rate`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec1692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1de62a",
   "metadata": {},
   "source": [
    "How would you go about aligning signals acquired at different rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc5ede",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Aligning neural activity and behavior to stimulus presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd5d2b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Binning continuous data </h4>\n",
    "\n",
    "One challenge in working with these data is that running, eye tacking, and neural activity are all sampled on separate data streams with different timestamps. This means that even though these data were all collected at the same time, there isn't necessarily a one-to-one matchup between timestamps in one data stream or other.\n",
    "\n",
    "The most common solution to this solution to this problem is data **resampling**. Typically timestamp bins are defined, and data are resampled into a common time stream. **What size bin should you use?** This depends on the timescale that is relevant to the analysis at hand.\n",
    "\n",
    "For today, we will be using stimulus-presentation bins to look at our data over a relatively large timescale. Specifically, we will use the `stimulus_response_df` that we generated above, which contains the mean response for each stimulus presentation, along with the `annotated_stimulus_presentations` table that we merged into it, which contains the mean running speed for each stimulus presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ca57b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "We are going to quantify the average `running_speed` and `pupil_width` during each stimulus presentation, and use those values to compare to the average cell activity for each stimulus presentation. \n",
    "\n",
    "This is equivalent to binning at the stimulus presentation frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88547f6b",
   "metadata": {},
   "source": [
    "#### Annotating `stimulus_presentations` with task events and behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd3b1e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "We can use the `brain_observatory_utilities` to annotate the `stimulus_presentations` table with information about what happened during each stimulus, including timing of `licks`, `rewards`, and whether the trial was a <b>hit</b> or a <b>miss</b> trial. \n",
    "\n",
    "It will also add the average `running_speed` and `pupil_width` for each stimulus presentation. These can be used to filter data, or plot directly against cell activity to ask about he relationship between running and neural activity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a89da",
   "metadata": {},
   "source": [
    "he `get_annotated_stimulus_presentations` function takes in the `ophys_experiment` object, which contains everything it needs to know about stimulus presentations, licks, rewards, running, etc., and returns an annotated version of the `stimulus_presentations` table.\n",
    "\n",
    "Let's go ahead and annotate our `stimulus_presentations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.behavior.data_formatting as behavior_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ed548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide dataset object to run the function\n",
    "annotated_stimulus_presentations = behavior_utils.get_annotated_stimulus_presentations(ophys_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb22ed",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What are the columns of the `annotated_stimulus_presentations` table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5f2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c4c9353",
   "metadata": {},
   "source": [
    "Great now we have the `mean_running_speed` and `mean_pupil_area` for each stimulus presentation.\n",
    "\n",
    "Next we need to align the cell traces to the stimulus presentation times, and compute the mean during each stimulus to compare to running and pupil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14dd7d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "#### Extracting stimulus aligned cell activity with `get_stimulus_response_df()`\n",
    "\n",
    "One challenge in extracting stimulus aligned activity is that the ophys data and stimulus presentations are sampled at different frequencies, as we saw above, so it is not possible to simply index from one into the other. \n",
    "\n",
    "If we want to compute stimulus and behavior aligned cell activity, we will need a way to associate ophys timestamps with the nearest stimulus timestamps. This can be tricky and time consuming.\n",
    "\n",
    "We can use the `get_stimulus_response_df` function from the `brain_observatory_utilities.datasets.optical_physiology.data_formatting` module to get the stimulus locked activity for all cells in the dataset. \n",
    "\n",
    "We already aligned `running_speed` and `pupil_width` to the stimulus onset times above, so we will then be able to link neural activity and behavior on a stimulus by stimulus basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e2dc9",
   "metadata": {},
   "source": [
    "\n",
    "To understand how the `get_stimulus_response_df` function  works in more detail, you can check the documentation, or go through the tutorial on how this function works here: https://github.com/AllenInstitute/brain_observatory_utilities/blob/main/example_notebooks/event_triggered_response_demo.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.optical_physiology.data_formatting as data_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dF/F traces aligned to image changes, and take the mean in a 1 second response window\n",
    "\n",
    "stimulus_response_df = data_formatting.get_stimulus_response_df(ophys_experiment, data_type='dff', event_type='changes',\n",
    "                                                            time_window=[-1, 2], response_window_duration=1)\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b3ca5",
   "metadata": {},
   "source": [
    "Note that the `stimulus_presentations_id` column is incuded in the table. This can be used to merge in the stimulus metadata from the `stimulus_presentations` table so that you can sort the cell responses by different stimulus and task conditions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485482c",
   "metadata": {},
   "source": [
    "Merge `stimulus_response_df` with `annotated_stimulus_presentations` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf357f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tables, and this time we will assign the results to the original variable name for `stimulus_response_df`\n",
    "stimulus_response_df = stimulus_response_df.merge(annotated_stimulus_presentations, on='stimulus_presentations_id')\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cde19",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What are the columns of your new `stimulus_response_df` after merging with `annotated_stimulus_presentations`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the columns available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb9be8",
   "metadata": {},
   "source": [
    "We now have the `mean_running_speed` and `mean_pupil_width` for each stimulus presentation, along with each neuron's `mean_response` in the same window after stimulus onset for each stimulus presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39598ff",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Using the `stimulus_response_df`, get the `mean_response` for one cell and plot `mean_running_speed` and `mean_response` against each other. \n",
    "\n",
    "Each point will represent one stimulus presentation during the task. \n",
    "\n",
    "Bonus: Color the data points based on other relevant information about each stimulus presentation, such as which `image_name` was shown, whether the `is_change` or `omitted` columns are True, or whether the trial was a `hit` or a `miss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stimulus response dataframe just for a particular cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ddaa49",
   "metadata": {},
   "source": [
    "Now that we have binned data, lets try plotting the relationship between running speed and the activity of our cell.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0201bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this cells mean response versus running speed for each stimulus presentation\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "sns.scatterplot(data=cell_df, x='mean_running_speed', y='mean_response', \n",
    "                hue='image_name', ax=ax)\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db597fb5",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Repeat using the `mean_pupil_width` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean pupil width across trials against this cells mean response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559db90d",
   "metadata": {},
   "source": [
    "Great! Our cell encodes both pupil diameter and running speed. \n",
    "\n",
    "You will recall, however, that these variables themselves are also correlated looked like they might have had a relationship to each other. \n",
    "\n",
    "Now that we have nicely binned data, try explicitly plotting the relationship between `mean_pupil_width` and `mean_running_speed`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22eb0a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the relationship between `mean_pupil_width` and `mean_running_speed`. \n",
    "\n",
    "How correlated are these two variables? Use the `pearsonr` function from the `scipy.stats` package to quantify the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e918e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pupil width and running speed binned by stimulus presentations against each other\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "sns.scatterplot(data=cell_df, x='mean_pupil_width', y='mean_running_speed', \n",
    "                hue='image_name', ax=ax)\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fb901",
   "metadata": {},
   "source": [
    "How correlated are our two variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation of X and y (pupil width and cell activity)\n",
    "\n",
    "pupil = cell_df.mean_pupil_width.values\n",
    "running = cell_df.mean_running_speed.values\n",
    "\n",
    "pearson_corr, pearson_pval = pearsonr(pupil, running)\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f337a",
   "metadata": {},
   "source": [
    "So...which of the factors best explains the variability in trial-to-trial responses of individual cells?\n",
    "\n",
    "In the next section, we will use Linear Regression to quantify the contribution of running and pupil to neural activity across trials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff51f6",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "<h3> Are cells tuned for running speed and pupil diameter? </h3>\n",
    "\n",
    "Studies have shown that animal movement and overall arousal state can influence the gain of sensory tuning. Running and other movements are also directly encoded by some neurons in the visual cortex, independent of stimulus identity. \n",
    "\n",
    "Let's use the Linear Regression techniques we learned in workshop 1 to evaluate single cell coding of running and pupil. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c63f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266f1e2",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "We will use the funciton below to fit and evaluate our models. Read through the documentation and the code in the function to understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidateLinearModel(X, y, n_split = 5, shuffle = False, shuffle_seed = None):\n",
    "    '''\n",
    "    Cross validate a linear model using KFold cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input data to fit\n",
    "    y : np.array\n",
    "        The output data to fit\n",
    "    n_split : int\n",
    "        The number of splits to use\n",
    "    shuffle : bool\n",
    "        Whether or not to shuffle the data\n",
    "    shuffle_seed : int\n",
    "        The seed to use for shuffling the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Mean Score: float\n",
    "        The average cross validation score\n",
    "    Model List: list    \n",
    "        The models fit to each fold of the data\n",
    "    Test score: np.array    \n",
    "        The cross validation scores for testing data each fold\n",
    "    Train score: np.array\n",
    "        The cross validation scores for testing data each fold\n",
    "    '''\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "        X = X.copy().reshape(-1,1)\n",
    "    # Initialize KFold object\n",
    "    folderizer = KFold(n_splits=n_split, shuffle=shuffle, random_state=shuffle_seed)\n",
    "    # Create an array to save the results\n",
    "    self_score = np.empty(n_split)\n",
    "    cross_score = np.empty(n_split)\n",
    "    models = [None]*n_split\n",
    "    # Loop through the folds, fit the model, and save the results\n",
    "    for ii, (train_index, test_index) in enumerate(folderizer.split(X, y)):\n",
    "        models[ii] = LinearRegression(fit_intercept=False).fit(X[train_index,:], y[train_index])\n",
    "        self_score[ii] = models[ii].score(X[train_index,:], y[train_index])\n",
    "        cross_score[ii] = models[ii].score(X[test_index,:], y[test_index])\n",
    "        \n",
    "    return np.mean(cross_score), models, cross_score, self_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282405fb",
   "metadata": {},
   "source": [
    "Note that the `KFold` object includes both `shuffle` and `shuffle_seed` parameters. `shuffle` does exactly what it sounds like- it randomizes the set data points included in each fold. `shuffle_seed` can be used to get reproducible results from this shuffling. This is particularly important if we want to compare models- using the same shuffle seed will give the same random set of trials across function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843441ce",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4> Which variables contribute to our model prediction? </h4>\n",
    "\n",
    "By fitting models with different combinations of features, we can get a richer sense of how different features are encoded by neural activity. \n",
    "\n",
    "One option for doing this is simply to fit a model to each variable of interest and compare their performance. This answers a very simple question: how much of a cells variability can be explained by this particular feature. We will see, however, that when variables are correlated the outcome of such one-at-a-time model fits can be difficult to interpret. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf143c17",
   "metadata": {},
   "source": [
    "First we will create an `X` matrix for each variable we want to test: stimulus, pupil, and running, along with a `y` matrix for the variable we want to predict: one cell's activity across trials. \n",
    "\n",
    "We can use the `mean_pupil_width`, `mean_running_speed` and `mean_response` columns of the `stimulus_response_df` for our cell to create one X matrix for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is our cell activity\n",
    "y = cell_df.mean_response.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7beb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a design matrix for images\n",
    "\n",
    "# initialize an empty matrix that is the length of the number of image presentations by 8, the number of unique images\n",
    "# Get the image presentations for all the change trials in our stim response df\n",
    "image_presentations = cell_df.image_name.values\n",
    "X_stim = np.zeros((len(image_presentations), 8))\n",
    "\n",
    "image_names = np.unique(image_presentations)\n",
    "# Loop through image presentations and set the column corresponding to that image index to 1 for each row\n",
    "for i, image_name in enumerate(image_presentations):\n",
    "    image_index = np.where(image_names==image_name)[0][0]\n",
    "    X_stim[i, image_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create design matrix for pupil & running\n",
    "X_pupil = cell_df.mean_pupil_width.values\n",
    "X_running = cell_df.mean_running_speed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are their shapes?\n",
    "print('X_stim:', X_stim.shape)\n",
    "print('X_pupil:', X_pupil.shape)\n",
    "print('X_running:', X_running.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053013ac",
   "metadata": {},
   "source": [
    "Now we will use our `crossValidateLinearModel` function to test how well each individual feature predicts this cell's activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7801d2",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `crossValidateLinearModel` function we created above to cross-validate and test our linear model for each of the variables separately. \n",
    "\n",
    "Which one produces the highest score? Use the `crossValidateLinearModel` function provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use our fancy new function to test a bunch of models.\n",
    "seed = 5\n",
    "# .... fill in the functions here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb6382",
   "metadata": {},
   "source": [
    "How does the model do on individual features? \n",
    "\n",
    "What about combinations of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d1a99",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Multiple linear regression - predicting activity based on a combination of features</h4>\n",
    "\n",
    "Instead of looking at the model fits for each feature individually, we can also create a model to explain neural activity based on a combination of features. To test the contribution of each feature, we can systematically remove each one to evaluate how much variance explained changes when that feature is missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d87dfe",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Create an X matrix that incorporates stimulus, pupil, and running, and evaluate performance of the model with all features included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack up the x matrices to make one big feature matrix\n",
    "# Note that pupil and running have to be reshaped to be able to stack with X_stim which has multiple columms already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide X_combo and y to the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ed0f9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Importantly, we cannot simply look at the model coefficients, as we did in the \"stimulus only\" example. This is because our model now contains different types of features with different magnitudes, and there is not a clear mapping between them. While the weights we fit will scale accordingly, they can no longer be directly compared. Visualizing the design matrix illustrates this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f886620",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the combined X matrix as a heatmap, zooming in on the first 20 stimulus presentations (i.e. rows). What are the columns of this matrix? \n",
    "\n",
    "What do each of the features look like? Do they have the same units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191d497",
   "metadata": {},
   "source": [
    "Here the columns are images 1-8, then running speed for each trial, then pupil width for each trial. \n",
    "\n",
    "The image features are one-hot encoded (i.e. represented by zeros or ones). Pupil and running are continuous variables. \n",
    "\n",
    "How might this influence our model fit? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e009d73",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "It is worth noting that `Scikit-Learn` has a tool called the `StandardScaler` that will normalize model inputs. This is useful if you want or need to more explicity compare coefficient weights across variables of different types, but we won't worry about that for now. You can read more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1ab5d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Leave one out test </h4>\n",
    "\n",
    "To test the encoding of any particular feature in a multiple linear regression model, we can compare the variance explained in the full model with reduced models with only a subset of the features. \n",
    "\n",
    "Specifically, we can systematically drop out feature one at a time and see how model performance changes. If the model gets worse, it suggests that this feature was explaining some of the variance in our data. Because other features are still included, this method is a way to avoid mistakenly assuming that a cell encodes all of a set of correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf6e2b",
   "metadata": {},
   "source": [
    "Let's create several design matrices, each with one of the variables left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create each design matrix as a stacked combo of all the features except the one we are leaving out\n",
    "X_wout_stimulus = np.hstack((X_pupil.reshape(-1,1), X_running.reshape(-1,1)))\n",
    "X_wout_running =  np.hstack((X_stim, X_pupil.reshape(-1,1)))\n",
    "X_wout_pupil =  np.hstack((X_stim, X_running.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b701f6f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `crossValidateLinearModel` function again to test model performance on each of the leave one out design matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a710d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6df4e",
   "metadata": {},
   "source": [
    "This cell really isnt well predicted by anything we have tested here... what about some other neurons? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa21d2e",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "<h3> Looking across the population </h3>\n",
    "\n",
    "Let's use the approach we just tried above, using the leave one out test, to assess which features lead to the largest drop in model prediction accuracy across all the neurons in the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6c2f7",
   "metadata": {},
   "source": [
    "First we need to build our X matrices for stimulus, running, and pupil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71355eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_stim\n",
    "\n",
    "# initialize an empty matrix that is the length of the number of image presentations by 8, the number of unique images\n",
    "# Get the image presentations for all the change trials in our stim response df\n",
    "image_presentations = cell_df.image_name.values\n",
    "X_stim = np.zeros((len(image_presentations), 8))\n",
    "\n",
    "unique_images = np.unique(image_presentations)\n",
    "# Loop through image presentations and set the column corresponding to that image index to 1 for each row\n",
    "for i, image_name in enumerate(image_presentations):\n",
    "    image_index = np.where(image_names==image_name)[0][0]\n",
    "    X_stim[i, image_index] = 1\n",
    "\n",
    "# Create design matrices for running and pupil\n",
    "X_pupil = cell_df.mean_pupil_width.values\n",
    "X_running = cell_df.mean_running_speed.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92262ecd",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Loop through all the cells in the dataset and use the `crossValidateLinearModel` function to quantify model performance on the full model, as well as each reduced model with one feature left out. \n",
    "\n",
    "Aggregate the model scores across cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd95c",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the distribution of model scores for each model that we fit - the full model with all features, and each reduced model with one of the features left out. \n",
    "\n",
    "Are the cells well fit by the full model? How about the reduced models? Which features are most strongly encoded by neurons in this ophys experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247423d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b323534c",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "<h4> Comparing across cell types </h4>\n",
    "\n",
    "Let's run the same analysis for a Vip experiment and see if there are any differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8da3b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> A note on cell types & behavior coding </h4>\n",
    "\n",
    "<p> Many studies have shown that different types of neurons have different encoding of sensory, behavioral, and task variables. \n",
    "For example, Somatostatin (Sst) and Vasoactive intestinal peptide (Vip) expressing inhibitory neurons in the visual cortex are known to show modulation by locomotion, arousal, attention, and learning. \n",
    "\n",
    "Here is a useful review on how animal behavior and learning influence the coding of different cell types in the visual cortex: \n",
    "https://doi.org/10.1016/j.conb.2018.04.020\n",
    "\n",
    "In addition, Sst and Vip inhibitory neurons are known to mutually inhibit each other and a shift in the balance between them can lead to disinhibition of excitatory neurons under certain conditions, such as with task engagement or novelty or arousal (all conditions that are included in the Visual Behavior Ophys dataset). \n",
    "\n",
    "In the <b>Visual Behavior Ophys</b> dataset, 3 different <b>transgenic mouse lines</b> were used to express GCaMP in either <b>excitatory neurons</b> (labeled by the Slc17a7-IRES2-Cre driver line), <b>Sst inhibitory neurons</b> (labeled by the Sst-IRES-Cre driver line), and <b>Vip inhibitory neurons</b>  (labeled by the Vip-IRES-Cre driver line). \n",
    "\n",
    "You can learn more about optical physiology and transgenic mouse lines in the <b>*DataBook*</b>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f26ec8",
   "metadata": {},
   "source": [
    "![cre_lines.png](../../resources/cre_lines.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f4a53",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Load the data for an experiment with `cre_line` = `Vip-IRES-Cre` using the `VisualBehaviorOphysProjectCache`.\n",
    "\n",
    "Plot the `max_projection` image to see what the cells look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ab21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an ophys experiment from the Vip-IRES-Cre transgenic line\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the max projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58b1b9",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `annotated_stimulus_presentations` table (which includes the `mean_running_speed` and `mean_pupil_width` for each stimulus presentation) using the `get_annotated_stimulus_presentations()` function from the `brain_observatory_utilities.datasets.behavior.data_formatting` modeule.\n",
    "\n",
    "Check the columns of this table to be sure you have the information you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40125f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide ophys_experiment dataset object to run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the colummns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26a9b",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now get the average cell responses using the `get_stimulus_response_df()` funciton from the `brain_observatory_utilities.datasets.optical_physiology.data_formatting` modeule.\n",
    "\n",
    "Merge the `stimulus_response_df` with the `annotated_stimulus_presentations`, using the `stimulus_presentations_id` column, as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stimulus aligned responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tables using their shared column, the stimulus_presentations_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e2ce0",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "First get the variable we are trying to predict - the stimulus aligned activity of one cell in the dataset. \n",
    "\n",
    "We can get this from the `stimulus_response_df`. \n",
    "\n",
    "Extract this information for one cell and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b7759",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now let's create our X matrices for stimulus, running speed, and pupil width, as well as a combined X matrix with all features. \n",
    "\n",
    "Create an X matrix for each reduced model as well, with one feature left out. We can compare this to the full model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ca78f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now loop through all cells and aggregate the model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83fb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5746180",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the distribution of model fits for all cells in this experiment, with one plot for the full model, and one for each of the reduced \"leave-one-out\" models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4b787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba068a47",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "<h4> Reflecting on our results & considering next steps </h4>\n",
    "\n",
    "How does this set of models do for the Vip cells compared to the Sst cells? \n",
    "\n",
    "Are there more cells that encode running or pupil?\n",
    "\n",
    "What would it look like for another experiment from the same `cre_line`? How variable is neural coding across animals? \n",
    "\n",
    "Are there differences cross `imaging_depth`? What about different `session_type`s?\n",
    "\n",
    "There are many fun projects that could be done using regression to quantify neural coding!! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.770791,
   "end_time": "2023-07-31T19:18:26.861555",
   "environment_variables": {},
   "exception": null,
   "input_path": "doc_template/examples_root/examples/nb/visual_behavior_load_ophys_data.ipynb",
   "output_path": "/tmp/tmpcq7t4kmr/scratch_nb.ipynb",
   "parameters": {
    "output_dir": "/tmp/tmpcq7t4kmr",
    "resources_dir": "/home/runner/work/AllenSDK/AllenSDK/allensdk/internal/notebooks/resources"
   },
   "start_time": "2023-07-31T19:17:20.090764",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
