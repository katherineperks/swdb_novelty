{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91dbe794-9558-4856-87b8-cdb9b3c29304",
   "metadata": {},
   "source": [
    "![Image](../../resources/cropped-SummerWorkshop_Header.png)\n",
    "\n",
    "<h1 align=\"center\">Population Coding</h1> \n",
    "<h2 align=\"center\"> SWDB 2024 - Day 2 - Afternoon Session </h2> \n",
    "<h3 align=\"center\"> Tuesday, August 20, 2024</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80800724-053b-4ed6-a4ac-5a55cf871419",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> \n",
    "\n",
    "### Questions  \n",
    "\n",
    "In the first workshop of today, we examined how sensory variables can be encoded in individual neurons' activity. \n",
    "\n",
    "We now turn our attention to the **coordinated activity of groups of neurons: population codes!**\n",
    "\n",
    "Key questions:\n",
    "* How do populations of neurons encode information about task-relevant sensory information? \n",
    "* How are these population codes modulated by task context or behavioral state? \n",
    "* What other types of thing are encoded in population activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1200e9-f52a-4211-9f21-a37d91283cd4",
   "metadata": {},
   "source": [
    "![neural_coding.png](../../resources/neural_coding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee20e0-953d-41ef-92bf-1d2bdf41c072",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### Extracellular Electrophysiology Data\n",
    "\n",
    "The data from in vivo extracellular electrophysiology experiments are organized into *sessions*, where each session is a distinct continuous recording period. During a session we collect:\n",
    "\n",
    "- spike times and characteristics (such as mean waveforms) from up to 6 neuropixels probes\n",
    "- local field potentials\n",
    "- behavioral data, such as running speed and eye position and lick times\n",
    "- visual stimuli which were presented during the session\n",
    "- cell-type specific optogenetic stimuli that were applied during the session\n",
    "\n",
    "The AllenSDK contains code for accessing across-session (project-level) metadata as well as code for accessing detailed within-session data. The standard workflow is to use project-level tools, such as `EcephysProjectCache` to identify and access sessions of interest, then delve into those sessions' data using `EcephysSession`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbc07a-4a27-493e-8ea1-5ccebc2db3fc",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Data access - loading an experiment of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c5c65-046b-4fd8-9b27-c227dbdf73f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to import these modules to get started\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import brain_observatory_utilities.datasets.behavior.data_formatting as behavior_utils\n",
    "\n",
    "# seaborn makes pretty plots & sets font sizes nicely\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'lines.markeredgewidth': 2})\n",
    "\n",
    "# magic functions for jupyter notebook plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61ddf2-987b-448f-982b-3a05949cb337",
   "metadata": {},
   "source": [
    "#### Load the cache and get metadata tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a01110-88b1-4980-8b74-6714e38090c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2024/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on CodeOcean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf7a58-11c1-4d1c-b115-489ac0b52226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import behavior projet cache class from SDK to be able to load the data\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorNeuropixelsProjectCache\n",
    "\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(cache_dir=data_root, use_static_cache=True)\n",
    "\n",
    "# if we needed to download the data we could have used the following line\n",
    "# cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c83164-f50a-4c49-bb10-8f36ca631001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the metadata tables\n",
    "units_table = cache.get_unit_table()\n",
    "\n",
    "channels_table = cache.get_channel_table()\n",
    "\n",
    "probes_table = cache.get_probe_table()\n",
    "\n",
    "behavior_sessions_table = cache.get_behavior_session_table()\n",
    "\n",
    "ecephys_sessions_table = cache.get_ecephys_session_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6aa9",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### Grab data from a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cache.get_ecephys_session(ecephys_session_id=1065437523)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28561c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "The stimulus presentations table is a record of every stimulus we presented to the mouse over the course of this experiment. Let's take a look at this table. \n",
    "    \n",
    "Here, we'll use an annotated version that includes some extra behavioral information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = behavior_utils.get_annotated_stimulus_presentations(session)\n",
    "stimulus_presentations.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85b1ca-9dc0-4139-9686-6b1bff3f8f05",
   "metadata": {},
   "source": [
    "#### It contains a great deal of information about the stimulus presentations! Let's look at all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e19fe-dae2-40c2-8f78-97a44a3dc178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sort(stimulus_presentations.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146314f-1d60-4ef9-9d63-a4c34d3da8ae",
   "metadata": {},
   "source": [
    "The experiment is divided into stimulus blocks. During each block a different set of stimuli are presented. A stimulus block can be active or passive. In active blocks, the mouse performs the change detection task introduced earlier. In passive blocks, there is no task.\n",
    "\n",
    "The different types of stimuli are indexed by the 'stimulus_block' column. Notice that our annotated stimulus table only has block 0, in which natural images are shown. What are the other stimulus blocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc20e0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">    \n",
    "\n",
    "What are all the types of stimulus block that were presented in this session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stimulus_presentations = session.stimulus_presentations\n",
    "all_stimulus_presentations.groupby('stimulus_block')[['stimulus_block', \n",
    "                                                'stimulus_name', \n",
    "                                                'active', \n",
    "                                                'duration', \n",
    "                                                'start_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af4f8f-1132-4eaf-9ce9-24a9a6902b9d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "What stimuli were shown in stimulus block 0? (Remember: our \"stimulus_presentations\" table already contains only this block.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba6ec2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58dc72d3",
   "metadata": {},
   "source": [
    "### Moving forwards, we will only look at this stimulus block, 0, where the mouse is performing the change detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d269df",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "How many stimulus presentations are there and how many image changes (in block 0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_stimulus_presentations = stimulus_presentations[stimulus_presentations['is_change']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8fdced5",
   "metadata": {},
   "source": [
    "### The (annotated) stimulus presentation table also includes information about the mouse behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66324f1",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "How well does the mouse do the task? What are its hit and miss rates? \n",
    "    \n",
    "(Note that the first few trials are auto-rewarded, and should not be counted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842d99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7200e0",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "    \n",
    "### Now let's get unit and channel data, sort the units by depth and filter for \"good\" units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get unit and channel data, sort the units by depth and filter for \"good\" units\n",
    "units = session.get_units() # contains information about spike waveforms, isolation quality\n",
    "channels = session.get_channels() # contains information about anatomical location\n",
    "\n",
    "unit_channels = units.merge(channels, left_on='peak_channel_id', right_index=True) # associate anatomical information with each unit\n",
    "\n",
    "#first let's sort our units by depth and filter\n",
    "unit_channels = unit_channels.sort_values('probe_vertical_position', ascending=False)\n",
    "\n",
    "#now we'll filter them\n",
    "good_unit_filter = ((unit_channels['snr']>1)&\n",
    "                    (unit_channels['isi_violations']<1)&\n",
    "                    (unit_channels['firing_rate']>0.1))\n",
    "\n",
    "good_units = unit_channels.loc[good_unit_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd98a46",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Which brain structures were recorded from in this session? How many units are present in each structure? (Hint: try the \"value_counts\" function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57950288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeae8c77",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### Let's look at the population activity in primary visual cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest = 'VISp'\n",
    "area_units = good_units[good_units['structure_acronym'] == area_of_interest]\n",
    "num_units = len(area_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1add6",
   "metadata": {},
   "source": [
    "### Let's start by looking at the neural activity! Does it reflect the image presentation?\n",
    "### The session.spike_times object contains all spike times, in seconds, indexed by the unit ID. Let's take a look at this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc06381",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times = session.spike_times\n",
    "print(type(spike_times))\n",
    "spike_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b3c71",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "\n",
    "Get the array of spike times for unit 1068230173. How many times does this unit spike in the first minute of the experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_spike_times = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af21712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90421fed",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "\n",
    "Plot a population spike raster spanning 1 second before to 1 second after a stimulus presentation. (Complete the two lines in the for loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot a single-trial raster, population PSTH, and representation matrix\n",
    "pre_time = 1\n",
    "post_time = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "presentation_idx = 1 # which trial to center the raster on\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx] # get spike times starting one second before this\n",
    "end_time = stimulus_presentations['end_time'][presentation_idx] # \n",
    "\n",
    "unit_num = 0\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu] # a numpy array\n",
    "    \n",
    "    unit_spike_times = \n",
    "    unit_num_spikes = \n",
    "    \n",
    "    ax.plot(unit_spike_times - start_time, unit_num*np.ones(unit_num_spikes,), 'k|', markersize=5)\n",
    "    unit_num += 1\n",
    "\n",
    "ax.set_title('Single-trial raster')\n",
    "ax.set_xlabel('Time relative to stimulus presentation (s)')\n",
    "ax.set_ylabel('Unit')\n",
    "ax.set_ylim((0, num_units+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a4acf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "Now let's compare to a change trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62007f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_idx = np.where(stimulus_presentations['is_change'].values)[0]\n",
    "presentation_idx = change_idx[0]\n",
    "\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx]\n",
    "end_time = stimulus_presentations['end_time'][presentation_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465c256",
   "metadata": {},
   "source": [
    "### Now let's take a look at the trial-averaged responses to see how a neuron encodes the stimulus in its time-dependent firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function to compute the PSTH\n",
    "def makePSTH(spikes, startTimes, windowDur, binSize=0.001):\n",
    "    bins = np.arange(0,windowDur+binSize,binSize)\n",
    "    counts = np.zeros(bins.size-1)\n",
    "    for i,start in enumerate(startTimes):\n",
    "        startInd = np.searchsorted(spikes, start)\n",
    "        endInd = np.searchsorted(spikes, start+windowDur)\n",
    "        counts = counts + np.histogram(spikes[startInd:endInd]-start, bins)[0]\n",
    "    \n",
    "    counts = counts/startTimes.size\n",
    "    return counts/binSize, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4818c39",
   "metadata": {},
   "source": [
    "Let's start by plotting the response of unit 0 to one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = stimulus_presentations['image_name'].unique()\n",
    "stimulus = stimuli[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "presentations = stimulus_presentations[stimulus_presentations['image_name'] == stimulus]\n",
    "num_presentations = len(presentations)\n",
    "\n",
    "start_times = presentations['start_time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = area_units.index\n",
    "iu = unit_ids[5]\n",
    "unit_spike_times = spike_times[iu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before_im = 1\n",
    "duration = 2\n",
    "\n",
    "unit_response, bins = makePSTH(unit_spike_times, \n",
    "                                  start_times - time_before_im, \n",
    "                                  duration, binSize=0.01)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(bins[:-1] - time_before_im, unit_response)\n",
    "ax.set_xlabel('Time from change (s)')\n",
    "ax.set_ylabel('Firing rate (Hz)')\n",
    "ax.set_title('Peri-stimulus time histogram for {}'.format(stimulus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa75e5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Plot the PSTHs for every unit to that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a311e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75fc0d5e",
   "metadata": {},
   "source": [
    "### We can see the trial structure of the task reflected in the PSTH. Some units have very strong transient responses to the image presentation. Do these responses depend on the task structure (whether the image is a change or not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8389f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Plot the PSTHs for every unit to that image on change trials only. Are the same neurons the most responsive on change trials as on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd6a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46dca101",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Plot the PSTHs for every unit to another image on change trials. Do the same neurons have the strongest responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3b043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f42a0c",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "    \n",
    "## Training a classifier on population spiking data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22746e5d-f67a-468f-84ef-9663e212ce3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we'll look at how the population activity encodes the image change. To determine how well we can decode the image change from population activity, we will train a **classifier** on a matrix of firing rates. Whereas regression models try to predict continuous values from the input features, classification models try to predict *labels* (also known as classes) from the input features.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "Let's start with a linear Support Vector Machine (SVM) classifier, which will try to draw linear boundaries between orientation conditions (the labels) in our high-dimensional firing rate space.\n",
    "\n",
    "This cartoon shows how we would expect an SVM to behave on a dataset with two dimensions and three conditions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091dd7ef",
   "metadata": {},
   "source": [
    "![SVM illustration](../resources/svm-classifier.png)\n",
    "\n",
    "SVM computes decision boundaries in feature space that can be used to classify different conditions. If a new data point appears, the SVM classifier will label it based on where it falls with respect to these boundaries.\n",
    "\n",
    "To train an SVM, we need to import the following methods from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f21c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2c41c",
   "metadata": {},
   "source": [
    "### First, we need to create a response matrix and vector of stimulus labels from the presentations that we'll decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ece525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total trials: {}'.format(len(stimulus_presentations)))\n",
    "print('Change trials: {}'.format(np.sum(stimulus_presentations['is_change'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f827e",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "The vast majority of stimulus presentations (~95%) are not a change. So a decoder could get 95% accuracy by predicting that there are no changes!\n",
    "\n",
    "To avoid this, we will balance the trials and decode change vs pre-change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ef76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add pre-change\n",
    "stimulus_presentations['pre_change'] = stimulus_presentations['is_change'].shift(-1)\n",
    "# broadcast hit/miss values from the change image to the pre-change\n",
    "stimulus_presentations.loc[stimulus_presentations['pre_change'], 'hit'] = stimulus_presentations[stimulus_presentations['is_change']].hit.values\n",
    "stimulus_presentations.loc[stimulus_presentations['pre_change'], 'miss'] =  stimulus_presentations[stimulus_presentations['is_change']].miss.values\n",
    "\n",
    "# isolate the trials to use for decoding\n",
    "decode_trial_ind = (stimulus_presentations['is_change'].values + stimulus_presentations['pre_change'].values)\n",
    "stimulus_presentations = all_stimulus_presentations[decode_trial_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed28e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check that the classes are balanced\n",
    "print(len(stimulus_presentations))\n",
    "print(np.sum(stimulus_presentations['is_change']))\n",
    "print(np.sum(stimulus_presentations['pre_change']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721c7f6-e280-4983-9049-95067ecfa2d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now let's make our matrix of responses and vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_response_array(spike_times, stimulus_presentations, units, window=.05):\n",
    "\n",
    "    '''\n",
    "    Create an array of spike counts x stimulus presentations, and a corresponding list of stimulus label\n",
    "    spike_times: spike times \n",
    "    stimulus_presentation: stimulus presentation table\n",
    "    units: units table containing only the units to get the responses of\n",
    "    '''\n",
    "\n",
    "    # sort spike times chronologically; necessary for the binary search later\n",
    "    sorted_spikes = dict()\n",
    "    for iu in units.index:\n",
    "        # mergesort/timsort since most spike_times are already sorted\n",
    "        sorted_spikes[iu] = np.sort(spike_times[iu], kind='mergesort')\n",
    "\n",
    "    # create our own copy of stimulus presentations and sort by presentation start time chronologically\n",
    "    # sortation of stimulus_presentations isn't necessary, but it speeds up the vectorized `searchsorted(...)`\n",
    "    stimulus_presentations = stimulus_presentations.sort_values(by='start_time', kind='mergesort', inplace=False)\n",
    "\n",
    "    # Calculate the duration of stimulus presentations, and drop NaN durations\n",
    "    stimulus_presentations['duration'] = stimulus_presentations['end_time'] - stimulus_presentations['start_time']\n",
    "    stimulus_presentations.dropna(subset='duration', inplace=True)\n",
    "    \n",
    "    # Warn if window size is too big\n",
    "    if np.any(window > stimulus_presentations['duration']):\n",
    "        print('Warning: window size longer than stimulus presentation')\n",
    "\n",
    "    responses_by_unit = list()\n",
    "    for iu in units.index:\n",
    "        unit_spike_times = sorted_spikes[iu]\n",
    "\n",
    "        # Determine the first and last spike time for each stimulus presentation\n",
    "        start_is = np.searchsorted(unit_spike_times, stimulus_presentations['start_time'])\n",
    "        end_is = np.searchsorted(unit_spike_times, stimulus_presentations['start_time']+window)\n",
    "\n",
    "        # presentation_spike_times = unit_spike_times[start_i:end_i]\n",
    "\n",
    "        # Calculate the response rate for each stimulus presentation\n",
    "        responses_by_unit.append((end_is - start_is) / stimulus_presentations['duration'])\n",
    "\n",
    "    # responses_by_unit has each row a unit, and each column a stimulus, flip so that rows are stimuli\n",
    "    responses = np.transpose(responses_by_unit)\n",
    "\n",
    "    # Extract the labels that match the responses from our sorted stimulus presentations table\n",
    "    labels = np.array(stimulus_presentations['image_name'])\n",
    "    \n",
    "    return responses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4284c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window=.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a10fb5",
   "metadata": {},
   "source": [
    "### We will first select a random subset of trials for training the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778764ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_presentations = responses.shape[0]\n",
    "num_train = int(total_presentations * 0.5) # Use 50% of trials for training\n",
    "random_trial_order = np.random.permutation(responses.shape[0])\n",
    "train_indices = random_trial_order[:num_train]\n",
    "\n",
    "training_data = responses[train_indices]\n",
    "training_labels = labels[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc65166",
   "metadata": {},
   "source": [
    "### Next, we'll create the model and fit it to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2735d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(responses[train_indices], labels[train_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7ef93",
   "metadata": {},
   "source": [
    "### Now that our model has been trained, we can ask it to classify unlabeled data (i.e., the sets of population firing rates that were not included in our original training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b96247",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = random_trial_order[num_train:]\n",
    "test_data = responses[test_indices]\n",
    "predicted_labels = clf.predict(responses[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260ff5d",
   "metadata": {},
   "source": [
    "### We can compare the predicted labels to the actual labels in order to assess the classifier's performance. We'll assess accuracy as the fraction of correctly predicted test images. As a baseline, we'll also compute the accuracy of a uniform random prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = np.unique(labels)\n",
    "\n",
    "actual_labels = labels[test_indices]\n",
    "accuracy = np.mean(actual_labels == predicted_labels)\n",
    "\n",
    "print('Accurary: {}'.format(accuracy))\n",
    "print('Chance level: {}'.format(1/len(conditions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbbe31",
   "metadata": {},
   "source": [
    "### We see that we perform better than chance! We can get a better sense of classification performance by using leave-one-out cross-validation. The `scikit-learn.model_selection.LeaveOneOut` iterator will automatically cycle through trials, on each iteration using one trial as a test and the others to train the classifier. Note that the model is fit independently on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b75b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in LeaveOneOut().split(responses):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses[train_indices], labels[train_indices])\n",
    "    \n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses[test_indices])\n",
    "    \n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "#     print(accuracy)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c31354",
   "metadata": {},
   "source": [
    "The leave-one-out cross-validation roughly agrees with our previous result. Do we do better on change or pre-change images? To assess this we'll look at the confusion matrix, which tells us how frequently each condition is predicted on change and pre-change presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusions, conditions, title=None):\n",
    "    \n",
    "    mean_confusion = np.mean(confusions, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    im = ax.imshow(mean_confusion, cmap='gray_r', clim=(0, 1))\n",
    "    plt.colorbar(im, ax=ax, label='Fraction of classifications')\n",
    "    \n",
    "    ax.set_xticks(range(len(conditions)), conditions, rotation=0)\n",
    "    ax.set_yticks(range(len(conditions)), conditions)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"Actual label\")\n",
    "    if title is None:\n",
    "        ax.set_title('Confusion Matrix')\n",
    "    elif type(title) is str:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d84f81",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "How does this V1-based decoder perform on hit trials vs miss trials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cdd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d56d5f",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "    \n",
    "## Exploring the time course of change-related information \n",
    "    \n",
    "Next we'll examine the time course of information in our population! Or more specifically: how the length of the spike count window affects the decoding accuracy. Can we decode the stimulus perfectly if we integrate spikes for long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa8734",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "First, let's try decoding with a longer response window of .2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c18775f3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "How long do we need to integrate spikes in order to saturate the decoding performance?\n",
    "    \n",
    "Here, we suggest using a relaxed, K-Fold cross-validation instead of LeaveOneOut for speed purposes. This divides the data set into K pieces, and iterates through them. On each of those K iterations, one piece is used as the \"test\" set and the others are used to train the decoder.\n",
    "\n",
    "Fill in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_lengths = np.arange(.01, .2, .02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "accuracies = np.zeros((len(window_lengths), num_splits))\n",
    "accuracies_hit = np.zeros((len(window_lengths), num_splits))\n",
    "accuracies_miss = np.zeros((len(window_lengths), num_splits))\n",
    "\n",
    "for i, window in enumerate(window_lengths):\n",
    "    print('{}/{}'.format(i, len(window_lengths)))\n",
    "\n",
    "    k = 0\n",
    "    for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0a904",
   "metadata": {},
   "source": [
    "How does this time course relate to the mouse's response times?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de494657",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "## Relationship between population size and decoding accuracy\n",
    "\n",
    "Next we'll examine how the size of the simultaneously recorded population affects decoding accuracy. In any physiology experiment, we only have a very small window into the overall population response. For example, there are about 500,000 neurons in mouse V1, so in this case we are measuring around 0.02% of the firing rates in this region.\n",
    "\n",
    "As the number of simultaneously recorded neurons increases, we expect that our ability to decode stimulus identity will improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82b8f8",
   "metadata": {},
   "source": [
    "To start with, let's try decoding with a random sample of 10 neurons. Note: here we're using the responses from our longest window above. So, if we used the full population we would be able to decode almost perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3502d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 10\n",
    "\n",
    "pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "responses_pop = responses[:, pop_idx]\n",
    "\n",
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")\n",
    "\n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b48d81",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "Does the result depend on which 10 neurons we sampled? Let's try another random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c85d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1967ae",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "\n",
    "Now, let's try to get a sense for how this changes with the number of neurons we use to train the classifier. \n",
    "    \n",
    "How many neurons do you need to decode with roughly 50% accuracy? 80%? 90%? Finish the code below.\n",
    "    \n",
    "For speed reasons, we again suggest using the KFold cross-validation strategy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sizes = np.arange(1, num_units+5, 5).astype('int')\n",
    "num_resamples = 10\n",
    "accuracies = np.zeros((len(pop_sizes), num_resamples, num_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf52811-159e-463c-9396-c48f2b101e5f",
   "metadata": {},
   "source": [
    "# With these analyses in hand, we leave you with some questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b30651-1d6b-4fa6-b7b5-e2edf37a182d",
   "metadata": {},
   "source": [
    "### If you integrate spikes in a fixed window length, how does the decoding accuracy depend on the time since the image presentation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc66128-5079-4d25-b384-bbf44e52cb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f79abf8-37d5-4dc5-8837-b1125ff7974c",
   "metadata": {},
   "source": [
    "### Where do the lick time distributions fall on the decoding accuracy vs time curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d810a3e-25fa-4959-9387-1afbc3885b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15d8f6ac-7d2d-47b1-8ce6-e3fcedf6d821",
   "metadata": {},
   "source": [
    "### Is the mouse's hit rate different for familiar or novel change images? Is the change decoding accuracy curve different for familiar vs novel change images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bfec1-285b-45be-b891-8044cb2ba1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f822a3bd-eb2c-4e32-819b-6d9c57a5b7fb",
   "metadata": {},
   "source": [
    "### Are the accuracy curves different in active vs passive blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f27f0-9909-4568-8636-02a2d835836a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4dd7ce-839a-49de-978e-ea9f836270a2",
   "metadata": {},
   "source": [
    "### Are other variables, including behavioral variables, also encoded in the population activity? Can you decode the running speed, pupil diameter, or licking behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0a050-362b-4a59-9d75-f062c38f2246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ead195-48d9-473b-9b6e-b4fc8edbbaf5",
   "metadata": {},
   "source": [
    "### What about in a different brain area? For example, is the change encoded in CA1 activity? What about in the joint activity across brain areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671b3a9-d56a-4815-a4a3-5fddebcacdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
