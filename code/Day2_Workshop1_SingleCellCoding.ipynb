{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1a8ea9",
   "metadata": {},
   "source": [
    "![cropped-SummerWorkshop_Header.png](../resources/cropped-SummerWorkshop_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e4bcd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1 align=\"center\"> Neural Encoding </h1> \n",
    "<h2 align=\"center\"> SWDB 2024 - Day 2 - Morning Session </h2> \n",
    "<h3 align=\"center\"> Tuesday, August 20, 2024</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a31e5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h2>Neural Encoding </h2>\n",
    "\n",
    "Neural coding describes how neurons represent information about the world. Coding can be studied by asking whether external or internal events lead to changes in neural activity (<b>encoding</b>), or by asking whether different types of information can be read out from neural activity (<b>decoding</b>). In this workshop we will address the question of neural encoding, with a focus on single cell encoding models. \n",
    "\n",
    "Encoding of <b>sensory</b> information has been studied for decades by presenting animals with stimuli and observing how the activity of individual neurons changes. To study encoding of <b>motor</b> variables, researchers train animals to perform a behavior (or observe naturalistic behaviors) and correlate the animals' movement with changes in neural activity. Recent research has demonstrated that even sensory areas have representations of motor and behavioral variables, and vice versa. This is often called \"multiplexed\" coding. Furthermore, neural encoding can be influenced by <b>cognitive</b> processes such as learning, task engagement, and decision making. \n",
    "\n",
    "The exact form of neural activity changes is also a part of the study of neural coding. Neurons can represent information based on their average firing rates over a period of time, based on the precise spike times relative to some event, using bursts of spikes, or based on synchrony and timing relative to global population activity. In this workshop, we will consider <b>average firing rates</b> as they relate to information coding.\n",
    "\n",
    "To learn more, check out this lecture Principles of Neural Coding by I. Memming Park: https://www.youtube.com/watch?v=DlFxUEdGlmQ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1103284",
   "metadata": {},
   "source": [
    "![neural_coding.png](../resources/neural_coding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0decd",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> \n",
    "\n",
    "This **morning's workshop** will focus on **how single cells encode information** in their average activity patterns (the problem of X-->R in the schemtic above). \n",
    "\n",
    "In the **afternoon workshop**, we will learn about **how to decode information from populations of neurons** (R-->X as shown above) and how variability and correlations influence decoding.\n",
    "\n",
    "A key question for both approaches is - ***what aspects of sensory, motor, or cognitive variables matter in the study of neural coding?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db894b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Behavior, attention, and learning shape sensory coding </h3>\n",
    "\n",
    "<p> Many studies over the last decade have shown that animal behavior, attention, and learning can influence neural coding even in sensory areas, often in a cell type specific way. Here are a few examples: \n",
    "\n",
    "* Animal movements, including locomotion, can alter the gain of sensory responses in the visual cortex **(Niell 2011; Darladat 2017)** and aross the brain **(Steinmetz 2019)**. \n",
    "* The influence of movement on sensory coding can be cell type specific **(DiPoppa, 2018)** and different populations of inhibitory neurons are jointly modulated by behavior and sensory features in distinct ways **(Millman 2020)**.\n",
    "* Attention and arousal (alertness), as indexed by pupil diameter, are associated with enhanced sensory coding through changes in neural correlations among specific cell types **(Poort 2022)**.\n",
    "* Sensory tuning can also be shaped by learning to associate a stimulus with a reward **(Shuler 2006, panel b)**, sharpening tuning and enhancing response reliability **(Poort 2018; Khan 2020)**. \n",
    "* Repeated experience with a sensory stimulus can lead to changes in neural representations, including developing predictive activity and expectation signals **(Fiser 2016)**\n",
    "* Novelty, surprise, and stimulus omission also reveal cell type specific differences in coding properties **(Garrett 2019, 2023)**.\n",
    "\n",
    "Here is a brief review of these findings: https://doi.org/10.1016/j.conb.2018.04.020 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0470d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "\n",
    "These types of modulations and coding changes often occur simultaneously as animals learn, move, attend, and interact with their environment both in the natural world and in a laboratory setting.\n",
    "\n",
    "In order to tease apart these various contributions to neural coding and brain function, it is necessary to:  \n",
    "1) Examine activity in behaving animals as they make choices and learn\n",
    "2) Link coding properties to the genetic identity and/or connectivity of neurons when possible, to put physiology in a circuit context\n",
    "3) Leverage computational approaches to quantify the contribution of different features and understand sources of variability\n",
    "\n",
    "\n",
    "**In this workshop, our first goal is to find a dataset that meets requirements 1 & 2, then we will learn some techniques for #3**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10643f",
   "metadata": {},
   "source": [
    "## Visual Behavior Ophys Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f2cf9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Visual Behavior Ophys: Why use this dataset? </h4>\n",
    "\n",
    "<p> To understand how neurons encode sensory and behavioral variables, it is useful to examine neural activity under conditions where sensory input and behavioral outputs vary across conditions and show rich and interesting structure that can be used to disentangle their unique contributions to neural signals. \n",
    "\n",
    "In the <b>Visual Behavior Ophys</b> dataset, neural activity was recorded while mice performed a visual change detection task. Mice were presented with stimuli and asked to make <b>choices</b> about those stimuli, while their <b>running</b> and <b>licking</b> behavior were measured, along with <b>pupil diameter</b> as a proxy for the animals' internal state. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a7c8e",
   "metadata": {},
   "source": [
    "![change_detection_task.png](../resources/change_detection_stimulus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22772e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Opportunities to study encoding of stimulus & behavior variables </h4>\n",
    "\n",
    "<p> In the Visual Behavior dataset, mice are trained to perform a visual change detection task. They view a series of sequentially presented visual stimuli and learn to lick a spout to earn a reward when they see the stimulus identity change. \n",
    "\n",
    "During the task, mice are free to run on a circular disc. Pupil diameter is also recorded, which can be used as a measure of overall behavioral state or arousal. When animals attend to a stimulus or are otherwise alert and active, the pupil dilates. In contrast, during quiet or inattentive states, the pupil constricts.\n",
    "\n",
    "Thus, in this dataset, we can ask about encoding of: \n",
    "* <b> Sensory stimuli</b> - via the images that are presented to the mouse, which can be <b>familiar or novel</b>\n",
    "* <b> Behavioral choice</b>  - whether the mouse licks or not following a given stimulus presentation\n",
    "* <b> Rewards</b>  - which are given depending on whether or not the mouse made a correct choice\n",
    "* <b> Locomotion & arousal</b>  - via changes in animal running speed or pupil diameter, as well as task engagement\n",
    "\n",
    "There may be additional dimensions of sensory or behavioral events that are of interest - can you think of any? \n",
    "\n",
    "Some other examples could be the number of exposures to a give image after a change, or the time since the last reward received by the mouse, or past trial outcomes, or perhaps a combination of running and pupil together is informative about cell activity.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0424e",
   "metadata": {},
   "source": [
    "![behavior_timeseries_color.png](../resources/behavior_timeseries_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7c358",
   "metadata": {},
   "source": [
    "There are some interesting dynamics here - how might they influence neural activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbfdb1",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Outline </h3>\n",
    "\n",
    "<h4> Part 1 -  Linking neural activity to stimulus & behavior</h4>\n",
    "\n",
    "* Examine cell activity during task performance\n",
    "* Aligning neural activity to events of interest\n",
    "* Tuning for stimulus & behavioral choice\n",
    "\n",
    "<h4> Part 2 -  Encoding of stimulus and behavioral choice </h4>\n",
    "\n",
    "* Building a design matrix & fitting a linear model\n",
    "* Splitting the data\n",
    "* Cross validation \n",
    "* Multiple linear regression - stimulus vs. choice\n",
    "\n",
    "<!-- <h4> Part 3 -  Coding for running speed and pupil diameter </h4>\n",
    "\n",
    "* Examining behavior variables\n",
    "* Binning continuous data\n",
    "* Leave one out test\n",
    "* Coding across the population\n",
    "* Comparing across cell types -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c6339",
   "metadata": {},
   "source": [
    "<div style=\"background: #e5e8e8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "## Part 1 - Linking neural activity to stimulus and behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb469bc",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Data access - loading an experiment of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import these modules to get started\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn makes pretty plots & sets font sizes nicely\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'lines.markeredgewidth': 2})\n",
    "\n",
    "# magic functions for jupyter notebook plotting\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2538f5",
   "metadata": {
    "papermill": {
     "duration": 0.044697,
     "end_time": "2023-07-31T19:17:31.563323",
     "exception": false,
     "start_time": "2023-07-31T19:17:31.518626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confirm that you are currently using the newest version of SDK (2.16.2)\n",
    "import allensdk\n",
    "allensdk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070402b",
   "metadata": {},
   "source": [
    "#### Use the AllenSDK's `VisualBehaviorOphysProjectCache` class to load the `ophys_experiment_table`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb45710",
   "metadata": {
    "papermill": {
     "duration": 0.017273,
     "end_time": "2023-07-31T19:17:37.435519",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.418246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use the same data access routines that we learned about on day 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fce480",
   "metadata": {
    "papermill": {
     "duration": 0.052759,
     "end_time": "2023-07-31T19:17:37.505024",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.452265",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2024/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on CodeOcean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import behavior projet cache class from SDK to be able to load the data\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache\n",
    "\n",
    "cache = VisualBehaviorOphysProjectCache.from_local_cache(cache_dir=data_root, use_static_cache=True)\n",
    "# if we needed to download the data we could have used the following line\n",
    "# cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c7558",
   "metadata": {
    "papermill": {
     "duration": 2.430735,
     "end_time": "2023-07-31T19:17:40.023206",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.592471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get table describing ophys experiments \n",
    "ophys_experiment_table = cache.get_ophys_experiment_table()    \n",
    "\n",
    "print('Number of ophys experiments = {}'.format(len(ophys_experiment_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4ef51",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the experiment we looked at yesterday and plot the max intensity projection. \n",
    "\n",
    "Note: it was the first one in the ophys experiment table,  `ophys_experiment_id` = 951980471\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33363898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd8953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecd6d86",
   "metadata": {},
   "source": [
    "#### Let's repeat what we did yesterday and plot all the cells' activity with the stimulus blocks overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc328977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using the cache\n",
    "ophys_experiment_id = 951980471\n",
    "ophys_experiment = cache.get_behavior_ophys_experiment(ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get all the data we need\n",
    "\n",
    "# Get normalized fluorescence traces\n",
    "dff_traces = ophys_experiment.dff_traces.copy()\n",
    "\n",
    "# Get deconvolved events\n",
    "events = ophys_experiment.events.copy()\n",
    "\n",
    "# Get timestamps \n",
    "ophys_timestamps = ophys_experiment.ophys_timestamps.copy()\n",
    "\n",
    "# Get stimulus presentations\n",
    "stimulus_presentations = ophys_experiment.stimulus_presentations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot events traces for all cells and plot stimulus blocks overlaid\n",
    "\n",
    "# Plot events for all cells\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, cell_specimen_id in enumerate(events.index.values): \n",
    "    ax.plot(ophys_timestamps, events.loc[cell_specimen_id]['events']+(i*2), color='gray')    \n",
    "\n",
    "# Iterate through stimulus blocks and show them in color\n",
    "colors = sns.color_palette()\n",
    "for i, stimulus_block_name in enumerate(stimulus_presentations.stimulus_block_name.unique()): \n",
    "    stimulus_block_data = stimulus_presentations[stimulus_presentations.stimulus_block_name==stimulus_block_name]\n",
    "    ax.axvspan(xmin=stimulus_block_data.start_time.values[0], xmax=stimulus_block_data.end_time.values[-1], \n",
    "               color=colors[i], alpha=0.25, label=stimulus_block_name)\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "ax.set_xlabel('Time in session (seconds)')\n",
    "ax.set_ylabel('Event magnitude (dF/F)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df12b77",
   "metadata": {},
   "source": [
    "These cells seem to care about different things. \n",
    "\n",
    "Some are active during the behavior session (orange), others are active during the natural movie (red). Some are more active during the gray screen (blue and green) than during the task. \n",
    "\n",
    "Why might cells show these differences in activity? What features of each part of the session could they care about? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31c43c",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Examining neural activity during task performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83bb98",
   "metadata": {},
   "source": [
    "#### Plot dF/F traces and events for one cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a7d67",
   "metadata": {},
   "source": [
    "Yesterday we looked at a cell with a very regular firing pattern, but we didnt actually check what it was responding to. \n",
    "\n",
    "Let's plot it again, then overlay stimulus and behavior information to see what it might care about. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6368b",
   "metadata": {},
   "source": [
    "Here's the plot from yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b602af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It happened to be the very first cell in the experiment, lets grab it's traces and plot them for a defined window of time\n",
    "cell_index = 0\n",
    "\n",
    "# Get its unique identifier, the cell_specimen_id\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "# The cell_specimen_id is the index to the traces tables\n",
    "dff_trace = dff_traces.loc[cell_specimen_id]['dff']\n",
    "events_trace = events.loc[cell_specimen_id]['events']\n",
    "\n",
    "# Pick the same 40 second window as yesterday\n",
    "window_start = 1000\n",
    "window_end = 1040\n",
    "\n",
    "# Plot events and dF/F, in seconds\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(ophys_timestamps, dff_trace, color='gray', label='dF/F')\n",
    "ax.plot(ophys_timestamps, events_trace, color=sns.color_palette()[0], label=events)\n",
    "ax.set_xlabel('Time in session (seconds)')\n",
    "ax.set_ylabel('dF/F')\n",
    "# Limit to our specified window\n",
    "ax.set_xlim(window_start, window_end)\n",
    "# Put the cell ID in the title so we know who this cell is\n",
    "ax.set_title('cell index: '+str(cell_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc881102",
   "metadata": {},
   "source": [
    "What is going on during the session when the cell is behaving this way? Is it responding to the stimulus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e5924",
   "metadata": {},
   "source": [
    "#### Now let's plot stimulus and behavior along with the cell trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092abec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions defined in day 1 intro\n",
    "\n",
    "def plot_running_speed_in_window(running_speed, window_start, window_end, ax): \n",
    "    '''\n",
    "    A function to plot running speed in specific window of time on the provided axis\n",
    "    \n",
    "    running_speed: A table including columns for `running_speed` and `timestamps`\n",
    "    '''\n",
    "    # Plot the running speed, in seconds, on the provided axis\n",
    "    ax.plot(running_speed.timestamps, running_speed.speed)\n",
    "    ax.set_ylabel('Running speed (cm/s)')\n",
    "    ax.set_xlabel('Time in session (seconds)')\n",
    "    # Limit to the selected window\n",
    "    ax.set_xlim(window_start, window_end)\n",
    "\n",
    "    # Return axes so we can add other things to it \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_licks_in_window(licks, window_start, window_end, ax): \n",
    "    '''\n",
    "    A function to plot licks occuring in specific window of time on the provided axis\n",
    "    \n",
    "    licks: A table with timestamps of each lick in the session\n",
    "    '''\n",
    "    # Get licks in the provided window\n",
    "    window_licks = licks[(licks.timestamps>=window_start) & (licks.timestamps<=window_end)]\n",
    "    # Iterate through them and plot as a line\n",
    "    for idx, lick in window_licks.iterrows():\n",
    "        ax.plot(lick.timestamps, -0.75, '|', color='gray')\n",
    "\n",
    "    # Return axes so we can add other things to it \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_rewards_in_window(rewards, window_start, window_end, ax): \n",
    "    '''\n",
    "    A function to plot rewards occuring in specific window of time on the provided axis\n",
    "    \n",
    "    rewards: A table with timestamps of each reward in the session\n",
    "    '''\n",
    "    # Get rewards in the provided window\n",
    "    window_rewards = rewards[(rewards.timestamps>=window_start) & (rewards.timestamps<=window_end)]\n",
    "    # Iterate through them and plot as a line\n",
    "    for idx, reward in window_rewards.iterrows():\n",
    "        ax.plot(reward.timestamps, -1, 'o', color='cyan')\n",
    "\n",
    "    # Return axes so we can add other things to it \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_stimuli_in_window(stimulus_presentations, window_start, window_end, ax): \n",
    "    '''\n",
    "    A function to plot a colored bar for each unique image_name in a specific window of time\n",
    "    within the provided stimulus_presentations table, on the provided axis. \n",
    "    \n",
    "    stimulus_presentations: Table of all stimulus presentations and associated metadata\n",
    "                            Function will limit to the `change_detection_behavior` block when image presentations occur\n",
    "    '''\n",
    "\n",
    "    # Make sure we are only looking at stimuli during the change detection block (other stimulus blocks do not have unique image names)\n",
    "    stimulus_presentations = stimulus_presentations[(stimulus_presentations.stimulus_block_name=='change_detection_behavior')]\n",
    "\n",
    "    # create colormap for unique image names\n",
    "    colors = sns.color_palette('hls', len(stimulus_presentations.image_name.unique()))\n",
    "    image_colors_dict = {}\n",
    "    for i, image_name in enumerate(np.sort(stimulus_presentations.image_name.unique())): \n",
    "        # omissions are white\n",
    "        if image_name == 'omitted': \n",
    "            image_colors_dict[image_name] = [1, 1, 1]\n",
    "        # images are in color\n",
    "        else: \n",
    "            image_colors_dict[image_name] = colors[i]\n",
    "\n",
    "    # Get all stimuli in the provided window\n",
    "    window_stimuli = stimulus_presentations[(stimulus_presentations.start_time>=window_start) & \n",
    "                                          (stimulus_presentations.end_time<=window_end)]\n",
    "\n",
    "    # Loop through stimuli and plot them\n",
    "    for idx, stimulus in window_stimuli.iterrows():\n",
    "        image_name = stimulus['image_name']\n",
    "        ax.axvspan(stimulus['start_time'], stimulus['end_time'], color=image_colors_dict[image_name], alpha=0.25)\n",
    "\n",
    "    # Return axes so we can add other things to it   \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make functions to plot a cell's dff or events trace\n",
    "\n",
    "def plot_cell_dff_trace_in_window(ophys_timestamps, dff_traces, cell_specimen_id, window_start, window_end, ax):\n",
    "    '''\n",
    "    A function to plot one cell's dF/F traces in a defined window, on the given axis\n",
    "    \n",
    "    ophys_timestamps: Array of timestamps corresponding to each ophys trace timepoint\n",
    "    dff_traces: A table with dff_traces for each cell_specimen_id in the experiment\n",
    "    cell_specimen_id: Which cell to plot\n",
    "    window_start: start time, in seconds, of window for plot\n",
    "    window_end: end time, in seconds, of window for plot\n",
    "    '''\n",
    "    # The cell_specimen_id is the index to the traces tables\n",
    "    dff_trace = dff_traces.loc[cell_specimen_id]['dff']\n",
    "\n",
    "    # Plot events and dF/F, in seconds\n",
    "    ax.plot(ophys_timestamps, dff_trace, color='gray', label='dF/F')\n",
    "    ax.set_xlabel('Time in session (seconds)')\n",
    "    ax.set_ylabel('dF/F')\n",
    "\n",
    "    # Limit to our specified window\n",
    "    ax.set_xlim(window_start, window_end)\n",
    "\n",
    "    # Put the cell ID in the title so we know who this cell is\n",
    "    ax.set_title('cell_specimen_id: '+str(cell_specimen_id))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cell_events_in_window(ophys_timestamps, events, cell_specimen_id, window_start, window_end, ax):\n",
    "    '''\n",
    "    A function to plot one cell's deconvolved events in a defined window, on the given axis\n",
    "    \n",
    "    ophys_timestamps: Array of timestamps corresponding to each ophys trace timepoint\n",
    "    events: A table with deconvolved events for each cell_specimen_id in the experiment\n",
    "    cell_specimen_id: Which cell to plot\n",
    "    window_start: start time, in seconds, of window for plot\n",
    "    window_end: end time, in seconds, of window for plot\n",
    "    '''\n",
    "    # The cell_specimen_id is the index to the traces tables\n",
    "    events_trace = events.loc[cell_specimen_id]['events']\n",
    "\n",
    "    # Plot events and dF/F, in seconds\n",
    "    ax.plot(ophys_timestamps, events_trace, color=sns.color_palette()[0], label=events)\n",
    "    ax.set_xlabel('Time in session (seconds)')\n",
    "    ax.set_ylabel('Event magnitude (dF/F)')\n",
    "\n",
    "    # Limit to our specified window\n",
    "    ax.set_xlim(window_start, window_end)\n",
    "\n",
    "    # Put the cell ID in the title so we know who this cell is\n",
    "    ax.set_title('cell_specimen_id: '+str(cell_specimen_id))\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac556fb",
   "metadata": {},
   "source": [
    "#### Plot neural activity, stimulus, and behavior for cell 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea25a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stimulus and behavior together in this window using our functions\n",
    "\n",
    "# Set cell specimen ID and window to plot\n",
    "cell_index = 0\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "\n",
    "window_start = 1000\n",
    "window_end = 1040\n",
    "\n",
    "# Create figure axis\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "\n",
    "# Plot dFF and events traces\n",
    "ax = plot_cell_events_in_window(ophys_timestamps, events, cell_specimen_id, window_start, window_end, ax)\n",
    "ax = plot_cell_dff_trace_in_window(ophys_timestamps, dff_traces, cell_specimen_id, window_start, window_end, ax)\n",
    "\n",
    "# Plot stimuli using function we created, for the subset of stimuli we selected\n",
    "ax = plot_stimuli_in_window(stimulus_presentations, window_start, window_end, ax)\n",
    "\n",
    "# Plot licks in the window we selected\n",
    "ax = plot_licks_in_window(ophys_experiment.licks, window_start, window_end, ax)\n",
    "\n",
    "# Plot rewards in the window we selected\n",
    "ax = plot_rewards_in_window(ophys_experiment.rewards, window_start, window_end, ax)\n",
    "\n",
    "ax.set_title('cell index: '+str(cell_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adab1ef",
   "metadata": {},
   "source": [
    "Well that sure is interesting! This cell appears to increase its activity just after the mouse licks! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817a0e2",
   "metadata": {},
   "source": [
    "Lets look at another time period for the same cell. Does the cell's activity depend on which image is being shown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cell specimen ID and window to plot\n",
    "cell_index = 0\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "\n",
    "window_start = 1860\n",
    "window_end = window_start+40\n",
    "\n",
    "# Create figure axis\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "\n",
    "# Plot dFF and events traces\n",
    "ax = plot_cell_events_in_window(ophys_timestamps, events, cell_specimen_id, window_start, window_end, ax)\n",
    "ax = plot_cell_dff_trace_in_window(ophys_timestamps, dff_traces, cell_specimen_id, window_start, window_end, ax)\n",
    "\n",
    "# Plot stimuli using function we created, for the subset of stimuli we selected\n",
    "ax = plot_stimuli_in_window(stimulus_presentations, window_start, window_end, ax)\n",
    "\n",
    "# Plot licks in the window we selected\n",
    "ax = plot_licks_in_window(ophys_experiment.licks, window_start, window_end, ax)\n",
    "\n",
    "# Plot rewards in the window we selected\n",
    "ax = plot_rewards_in_window(ophys_experiment.rewards, window_start, window_end, ax)\n",
    "\n",
    "ax.set_title('cell index: '+str(cell_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e60462",
   "metadata": {},
   "source": [
    "The cell doesnt seem to care about which image is shown, but it sure does like it when the mouse licks! It also seems to care about omissions...\n",
    "\n",
    "Based on what we have seen in the raw data, you would expect this cell to have a big difference in activity for **hit** trials (when the mouse correctly licks after a change) versus **miss** trials (when there is no lick), and minimal selectivity for image identity. It also may be driven by stimulus omissions, which are an unexpected absence of sensory input in this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ad73e",
   "metadata": {},
   "source": [
    "Let's try another cell and see what it responds to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ea46d",
   "metadata": {},
   "source": [
    "#### Plot neural activity, stimulus, and behavior for cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d45690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cell specimen ID and window to plot\n",
    "cell_index = 6\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "\n",
    "window_start = 1860\n",
    "window_end = window_start+40\n",
    "\n",
    "# Create figure axis\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "\n",
    "# Plot dFF and events traces\n",
    "ax = plot_cell_events_in_window(ophys_timestamps, events, cell_specimen_id, window_start, window_end, ax)\n",
    "ax = plot_cell_dff_trace_in_window(ophys_timestamps, dff_traces, cell_specimen_id, window_start, window_end, ax)\n",
    "\n",
    "# Plot stimuli using function we created, for the subset of stimuli we selected\n",
    "ax = plot_stimuli_in_window(stimulus_presentations, window_start, window_end, ax)\n",
    "\n",
    "# Plot licks in the window we selected\n",
    "ax = plot_licks_in_window(ophys_experiment.licks, window_start, window_end, ax)\n",
    "\n",
    "# Plot rewards in the window we selected\n",
    "ax = plot_rewards_in_window(ophys_experiment.rewards, window_start, window_end, ax)\n",
    "\n",
    "ax.set_ylim(-1.5, 5.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc580f",
   "metadata": {},
   "source": [
    "In contrast to cell 0, which was active when the mouse licked, cell 6 responds to visual stimuli, and seems to be pretty selective for the specific identity of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a5b7a",
   "metadata": {},
   "source": [
    "To **quantify cell activity** in response to different stimulus or task events, it would be helpful to **compute the mean response** in a given time window after each event, so that we have a single value representing each cell's activity for each image presentation. \n",
    "\n",
    "We can then use the array of cell activity across trials as an **input to a linear model** to see if we can **predict the cell's trial by trial activity** based on different **task features**, like the stimulus identity or whether or not the mouse licked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc5ede",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Aligning neural activity to events of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448ec6c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "We now have two interesting cells that might have unique coding properties. How can we quantify what these cells encode?\n",
    "\n",
    "One way is to simply plot an **image tuning curve**, or compare the response to **hits and misses**, to evaluate how different the cells responses are to different conditions, and quantify the variability in that response. Tuning curves can be considered as a simple model of cell activity. \n",
    "\n",
    "To compute a tuning curve, first we need to align neural activity to the times of stimulus presentations, and compute the average response for each stimulus.\n",
    "\n",
    "Let's explore our `stimulus_presentations` table and see what information we have available to link to cell activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78703e21",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `stimulus_presentations` attribute of the `ophys_experiment` data object. \n",
    "\n",
    "What are the columns of the `stimulus_presentations` table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15c957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e98180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5144d9",
   "metadata": {},
   "source": [
    "The `stimulus_presentations` table includes information about what stimuli were presented during the session, indicated by the `image_name` column, and whether or not the stimulus was a change (`is_change`) or omitted (`is_omitted`) or whether it was a novel image (`is_image_novel`), which occurs in some sessions (but not the one we are looking at now).\n",
    "\n",
    "However this table doesn't tell us **whether the mouse licked or not** for each stimulus, which we need to know to determine if a trial was a `hit` or a `miss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88547f6b",
   "metadata": {},
   "source": [
    "#### Annotating `stimulus_presentations` with task events and behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd3b1e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "How can we add information about behavior events to our `stimulus_presentations` table?\n",
    "\n",
    "We know from the day 1 overview of the Visual Behavior dataset that there are also tables for `licks`, `rewards`, and `trials`, but linking all these tables together seems like a lot of work...\n",
    "\n",
    "Fortunately, the `brain_observatory_utilities` package provides a useful tool to annotate the `stimulus_presentations` table with information about what happened during each stimulus, including timing of `licks`, `rewards`, and whether the trial was a <b>hit</b> or a <b>miss</b> trial. \n",
    "\n",
    "It will also add the average `running_speed` and `pupil_width` for each stimulus presentation. These can be used to filter data, or plot directly against cell activity to ask about he relationship between running and neural activity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a89da",
   "metadata": {},
   "source": [
    "The `get_annotated_stimulus_presentations` function can be found in the `datasets.behavior.data_formatting` module of `brain_observatory_utilities`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.behavior.data_formatting as behavior_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3eb7f",
   "metadata": {},
   "source": [
    "The `get_annotated_stimulus_presentations` function takes in the `ophys_experiment` object, which contains everything it needs to know about stimulus presentations, licks, rewards, running, etc., and returns an annotated version of the `stimulus_presentations` table.\n",
    "\n",
    "Let's go ahead and annotate our `stimulus_presentations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ed548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide dataset object to run the function\n",
    "annotated_stimulus_presentations = behavior_utils.get_annotated_stimulus_presentations(ophys_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb22ed",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What are the columns of the `annotated_stimulus_presentations` table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37f671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d4966a",
   "metadata": {},
   "source": [
    "Now the `annotated_stimulus_presentations` table includes useful things like whether the trial was a hit or miss, the mean running speed, whether the mouse was engaged or not, and more.\n",
    "\n",
    "The next step is to link the stimulus and behavior events with neural activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c9353",
   "metadata": {},
   "source": [
    "#### Extracting stimulus aligned cell activity with `get_stimulus_response_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14dd7d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "One challenge in extracting stimulus aligned activity is that the ophys data and stimulus presentations are sampled at different frequencies, so it is not possible to simply index from one into the other (you can check the `metadata` attribute of the `ophys_experiment` object to see what the frame rates were for the session we are looking at)d. \n",
    "\n",
    "If we want to compute stimulus aligned cell activity, we will need a way to associate ophys timestamps with the nearest stimulus timestamps. This can be tricky and time consuming.\n",
    "\n",
    "Fortunately, the `brain_observatory_utilities` package provides tools to make this easier. \n",
    "\n",
    "We can use the `get_stimulus_response_df` function from the `datasets.optical_physiology.data_formatting` module to get the stimulus locked activity for all cells in the dataset. \n",
    "\n",
    "The `get_stimulus_response_df` function works by finding the nearest timepoint in the `ophys_timestamps` for every `start_time` in the `stimulus_presentations` table, then extracts a snippet of each cell's activity around each stimulus start time. \n",
    "\n",
    "To understand how this function works in more detail, you can check the documentation, or go through the tutorial on how this function works here: https://github.com/AllenInstitute/brain_observatory_utilities/blob/main/example_notebooks/event_triggered_response_demo.ipynb \n",
    "\n",
    "For now, we are going to go ahead and plug in our data and see what comes out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.optical_physiology.data_formatting as data_formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9afc20",
   "metadata": {},
   "source": [
    "The function below will extract stimulus aligned traces for a specific `data_type` (here we will use `dff`) for a given `event_type` (here we will use `changes`).\n",
    "    \n",
    "It will extract the traces in a defined `time_window` around each event (here we will use -1 to 2 seconds around the change times).\n",
    "\n",
    "It will also compute the `mean_response` in a window after each event time, defined by `response_window_duration`. \n",
    "Since we saw above that cell 0 was active when licks occured after image onset, and we know that licks happen ~0.5 seconds after image presentations, \n",
    "let's use a 1 second response window to capture the mean activity following each image change (and associated licks).\n",
    "\n",
    "The first input to this function is the `ophys_experiment` data object we have been working with, which contains all the data and metadata that the function needs to compute stimulus aligned responses, including the `dff_traces` and `stimulus_presentations` table.\n",
    "\n",
    "You can check the documentation for this function to learn more about how it works using `data_formatting.get_stimulus_response_df?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dF/F traces aligned to image changes, and take the mean in a 1 second response window\n",
    "\n",
    "stimulus_response_df = data_formatting.get_stimulus_response_df(ophys_experiment, data_type='dff', event_type='changes',\n",
    "                                                            time_window=[-1, 2], response_window_duration=1)\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b3ca5",
   "metadata": {},
   "source": [
    "Note that the `stimulus_presentations_id` column is incuded in the table. This can be used to merge in the stimulus metadata from the `stimulus_presentations` table so that you can sort the cell responses by different stimulus and task conditions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89b57f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Check the index and columns of `stimulus_response_df` and `stimulus_presentations`. Which columns do they have in common? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92b09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708897c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c485482c",
   "metadata": {},
   "source": [
    "#### Merge `stimulus_response_df` with `annotated_stimulus_presentations` table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e748d1",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "\n",
    "We can combine our tables of stimulus aligned cell activity and stimulus metadata by merging the `stimulus_response_df` with the `annotated_stimulus_presentations` table using the `stimulus_presentations_id` column, which they have in common. \n",
    "\n",
    "Pandas `.merge()` is a very useful method that can be called on any dataframe. The input to the method is another dataframe, and the name of one or more columns that are present in both dataframes (provided as a list to the `on` argument - i.e. what column to merge on). The value to merge on can also be the index of the dataframes. \n",
    "\n",
    "If the two dataframes do not have any columms or indices in common, they cannot be merged. If the two dataframes have multiple colums with the same name, but only one of the columns is provided in the `on` argument, both of the redundant columns will be kept, and an '_x' and '_y' will be added to the column names to distinguish them.  \n",
    "\n",
    "Check the documentation for the `pandas.merge` function to learn more and see some examples: https://pandas.pydata.org/docs/reference/api/pandas.merge.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77127cab",
   "metadata": {},
   "source": [
    "Let's call the `merge()` method on the `stimulus_response_df` and provide it with the `annotated_stimulus_presentations` table, and instruct it to merge on the thing they have in common - `stimulus_presentations_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf357f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tables, and this time we will assign the results to the original variable name for `stimulus_response_df`\n",
    "stimulus_response_df = stimulus_response_df.merge(annotated_stimulus_presentations, on='stimulus_presentations_id')\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cde19",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What are the columns of your new `stimulus_response_df` after merging with `annotated_stimulus_presentations`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40eb9be8",
   "metadata": {},
   "source": [
    "Great, now we have useful information like `image_name` and `hit` that we can sort the cell responses by, along with the `mean_running_speed` and `mean_pupil_width` for each stimulus presentation. These will come in handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02862e72",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Tuning for stimulus and behavioral choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc149f",
   "metadata": {},
   "source": [
    "Now we can plot image tuning curves using the `mean_response` for each `image_name` in our `stimulus_response_df`. \n",
    "\n",
    "We can separate them by whether the trial was a `hit` or a `miss` to see whether the animal's choice influences cell activity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb5514",
   "metadata": {},
   "source": [
    "#### Image tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b709474",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "For the first cell we looked at earlier (cell index = 0), plot the average response for each `image_name`. \n",
    "\n",
    "You can do this by taking the average of the `mean_response` column of the `stimulus_response_df` for each `image_name`. \n",
    "\n",
    "Bonus: Pandas `.groupby()` method can make this easier (but is not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda75f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b54481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1808ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb73cff",
   "metadata": {},
   "source": [
    "Is the cell image selective? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf09979",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Make this plot again, but for the second cell we looked at above (cell_index = 6), which appeared to be somewhat image tuned in the raw data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471540a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7ad55ba",
   "metadata": {},
   "source": [
    "Is this cell image selective? \n",
    "\n",
    "How do we know if the average response is a reasonable estimate of the cell's activity? \n",
    "\n",
    "How variable are the responses across trials? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92286a1",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "#### Trial to trial variability \n",
    "\n",
    "While the average tuning curve of a neuron may give the appearance of image selectivity, it is important to understand how reliable the cell's responses are across trials, which tells us whether the tuning curve is an accurate model of the cell's activity. \n",
    "\n",
    "Below we will plot cell activity across trials, in addition to the mean, first for **cell 0** (which had licking related activity) then for **cell 6** (which appeared to be image selective in the raw data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08d2de",
   "metadata": {},
   "source": [
    "Let's plot the cell responses on each individual trial, using the `mean_response` column of the `stimulus_response_df` (which as you recall is the mean of the dF/F trace in a 1 second window after stimulus onset), along with the trial averaged response. \n",
    "\n",
    "How variable is the cell activity across repeated presentations of a given image?\n",
    "\n",
    "Here we will use `seaborn.scatterplot` function with the `stimulus_response_df` dataframe for this cell as the input: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "\n",
    "Seaborn is a magical tool for making all kinds of plots using pandas dataframes. It is built on top of matplotlib, so you can use both together by passing around maptlotlib axes handles, as we do below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean response for each image and the variability about tha response\n",
    "\n",
    "# Get responses for our first cell (index 0) that showed activity after the mouse licked\n",
    "cell_index = 0\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "\n",
    "# Get cell data and compute tuning curve using groupby\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "tuning_curve = cell_df.groupby(['image_name']).mean()[['mean_response']]\n",
    "\n",
    "# Get sorted image names for x-axis\n",
    "image_names = np.sort(tuning_curve.index.values)\n",
    "\n",
    "# Make the plot\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_names, color='r', linestyle='None', ax=ax)\n",
    "ax = sns.scatterplot(data=cell_df, x='image_name', y='mean_response', ax=ax, zorder=0)\n",
    "ax.set_title('cell index: '+str(cell_index)+'\\nImage selectivity')\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e266420",
   "metadata": {},
   "source": [
    "Let's look at our other cell (cell index 5), which appeared to be more reliably selective in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean response for each image and the variability about tha response\n",
    "\n",
    "# Get responses for our second cell (cell index = 5) that showed some degree of image tuning\n",
    "cell_index = 6\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "\n",
    "# Get cell data and compute tuning curve using groupby\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "tuning_curve = cell_df.groupby(['image_name']).mean()[['mean_response']]\n",
    "\n",
    "# Get sorted image names for x-axis\n",
    "image_names = np.sort(tuning_curve.index.values)\n",
    "\n",
    "# Make the plot\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_names, color='r', linestyle='None', ax=ax)\n",
    "ax = sns.stripplot(data=cell_df, x='image_name', y='mean_response', ax=ax, zorder=0)\n",
    "ax.set_title('cell index: '+str(cell_index)+'\\nImage selectivity')\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ae6e",
   "metadata": {},
   "source": [
    "There are some outliers, but the responses are generally more reliable within a given image for cell 6 compared to cell 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d8fa0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "### Choice tuning\n",
    "\n",
    "What could explain this trial to trial variability? Could it be that the strength of the cell response differs depending on whether a trial was a hit or a miss? \n",
    "\n",
    "For cell 0, which we saw had increased activity when the mouse licked, we could predict that it would be more active for hit trials (when the mouse licks) compared to miss trials (where the mouse fails to lick). \n",
    "\n",
    "For cell 6, we saw some image selectivity, but we couldn't tell from the raw activity whether this cell cares about licking or rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46b5a0",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the image tuning curves separated by whether the trials were a `hit` or not for both **cell 0** and **cell 6**\n",
    "\n",
    "Note: seaborn's `pointplot` function can make it easy to plot the image tuning curves split by hit and miss, using the `hue` argument. \n",
    "\n",
    "Also, its helpful to provide the sorted values of the x-axis via the `order` argument to `pointplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae2a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d259d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53728867",
   "metadata": {},
   "source": [
    "Are these cell's activity influenced by the mouse's choice? Do they care about both images and choice or just one or the other?\n",
    "\n",
    "Can we model the cell's activity in a more quantitative way, and determine the unique contribution of each feature? \n",
    "\n",
    "How much of the cell's variance is explained by the stimulus, and how much is explained by the mouse's choice? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73874f",
   "metadata": {},
   "source": [
    "<div style=\"background: #e5e8e8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "## Part 2 - Encoding of stimulus & behavioral choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee4907",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4>How well can you predict neural activity based on stimulus information? \n",
    "<p>\n",
    "\n",
    "How much variance is explained by stimulus versus choice?</h4>\n",
    "<p>\n",
    "One of the useful things about regression models is that that can be used evaluate the role of different kinds of features in predicting data. \n",
    "\n",
    "Earlier, we saw that examples of how stimulus identity can be encoded by the activity of a single neuron, and that the animal's behavioral choice can also have an influence. Here, we will recast this tuning problem as a regression problem, allowing us to use our regression tool-box to understand this tuning. \n",
    "\n",
    "We will then see how using a common modeling framework allows us to quantitatively compare the encoding of different features by analyzing the contribution of each feature to explaining variance in neural activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640cbfd",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Linear regression: the math </h4>\n",
    "\n",
    "    \n",
    "In a regression problem, we are have pairs of data points  $(𝑥⃗_𝑖,𝑦_𝑖)$\n",
    "  where  𝑖∈[1,𝑁]. We want to develop a function  $𝑓(𝑥⃗ )$\n",
    "  such that  $𝑓(𝑥⃗_𝑖)≈𝑦_𝑖$\n",
    "  for each pair of points in the data set.\n",
    "    \n",
    "    \n",
    "The simplest regression problem is linear regression, in which we try to create the function $f$ by linearly combining a set of functions that act on the points $x$.\n",
    "\n",
    "$f(\\vec{x}_i) = \\sum_j w_j \\phi(\\vec{x}_i)$\n",
    "\n",
    "The functions $\\phi(\\vec{x})$ are chosen according to the question you are trying to answer. They are often called \"features\".  \n",
    "    \n",
    "The coefficients $w_j$ are called \"weights.\" When we talk about fitting a regression model, what we mean is determining the best set of weights for our  $𝑓(𝑥⃗_𝑖) \\rightarrow 𝑦_𝑖$ mapping? \n",
    "\n",
    "\n",
    "But what is the \"best\" set of weights? We try to choose the weights that minimize overall error between $f(x)$ and $y$.In the case of linear regression we use the sum of squared residuals between our for each $𝑓(𝑥⃗ 𝑖)$ and the corresponding $y_i$:\n",
    "\n",
    "$E = \\frac{1}{2} \\sum_i \\left | y_i - f\\left ( \\vec{x}_i \\right ) \\right |^2 = \\frac{1}{2} \\sum_i \\left | y_i - \\sum_j w_j \\phi (\\vec{x}_i ) \\right |^2 $\n",
    "\n",
    "\n",
    "\n",
    "This particular problem has an exact analytic solution that is easy to implement, but in this tutorial, we will look at how to perform regression using the `scikit-learn` Python package.  `scikit-learn` has many regression algorithms in common use built in, most of which do not have simple analytic solutions.  In addition, other packages have adopted the `scikit-learn` style interface.  One advantage of this is that multiple algorithms can be deployed with the same code.\n",
    "\n",
    "\n",
    "\n",
    "The `scikit-learn` website:  http://scikit-learn.org/stable/\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e3ce",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4> Linear regression: in words </h4>\n",
    "\n",
    "The goal of regression is to <b>predict</b> a variable `y` based on another variable, `X`. \n",
    "\n",
    "In our case, we want to predict the cell response as `y` for each image, represented by `X`\n",
    "\n",
    "`X` can be continuous (as in fitting a line, y vs. X), or categorical, as it is in the case of image tuning. \n",
    "\n",
    "To fit the model, we need to have one value of `y` measured for each value of `X`. Then, using our model, we can extrapolate beyond our original data to predict what unmeasured values of `y` might look like for any given value of `X`.\n",
    "\n",
    "To predict `y` from `X` we need to create what is called a <b>\"design matrix\" (aka \"feature matrix\")</b>, which is just a matrix that contains the value of the categorical variable `X` corresponding to each measurement of the thing we are trying to predict, `y`. \n",
    "\n",
    "When there is more than one value for `X` (as in the case of image names), we can use \"one-hot\" encoding to represent it, which just means that we assign a value of 0 or 1 for each of the 8 image names to encode which of the 8 images was shown for a given trial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e302dba",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Building a design matrix & fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf92b6",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Let's build out a X design matrix representing the 8 image stimuli using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08491492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stimulus responses for our cell\n",
    "cell_index = 0\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "\n",
    "# Get the image presentations for all the change trials in our stim response df\n",
    "image_presentations = cell_df.image_name.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71109b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty matrix that is the length of the number of image presentations by 8, the number of unique images\n",
    "X = np.zeros((len(image_presentations), 8))\n",
    "\n",
    "image_names = np.unique(image_presentations)\n",
    "# Loop through image presentations and set the column corresponding to that image index to 1 for each row\n",
    "for i, image_name in enumerate(image_presentations):\n",
    "    image_index = np.where(image_names==image_name)[0][0]\n",
    "    X[i, image_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0eb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - there is a trick for doing this, using numpy's unique function\n",
    "\n",
    "# By setting the \"return_inverse\" function of the unique command, you will return\n",
    "# (1) the list of unique image names and (2) index into that list of each image presentation\n",
    "image_names, image_index = np.unique(image_presentations, return_inverse=True)\n",
    "\n",
    "# Create an array the length of stimulus presentations by 8 (the number of images)\n",
    "X = np.zeros((len(image_index),8))\n",
    "# Loop through image indices and build up the X matrix \n",
    "for ii in range(len(image_index)):\n",
    "    X[ii, image_index[ii]]  = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce3617",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the design matrix (aka feature matrix). Label the axes. \n",
    "\n",
    "Plot the full matrix, and also the matrix limited to the first 20 trials so you can actually see whats going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096fa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f18c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb41ff47",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "What about the `y` part of the equation? What is the thing we are trying to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b3072",
   "metadata": {},
   "source": [
    "We want to predict the cell's activity on each trial, based on which image was shown. \n",
    "\n",
    "`y` is the mean response for each trial for one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses of one cell for the same set of trials represented in X\n",
    "y = cell_df.mean_response.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.plot(y, 'o')\n",
    "ax.set_ylabel('dF/F')\n",
    "ax.set_xlabel('Trial #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c73dc",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now that we have our feature matrix, X, using using it to fit a model is quite simple. We can import the `scikit-learn` package (we will call it \"sklearn\" to save some typing). It has a nice interface for fitting regression models has a variety of useful features that we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c63f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f98dd",
   "metadata": {},
   "source": [
    "The sklearn interface is object oriented. This means that to fit a model, we need to instantiate a \"LinearRegression\" object. We will then use this for to handle our fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LinearRegression object\n",
    "# We use fit_intercept=False because this is a categorical scenario, not predicting a continuous variable\n",
    "lr = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d64a26",
   "metadata": {},
   "source": [
    "You will have notice that we used a specific setting when we created the `LinearRegression` object, `fit_intercept=False`. This prevents `LinearRegression` from fitting the constant/intercept term in our model. \n",
    "\n",
    "Since we are using a categorical variable (the image identity) to predict activity, rather than a continous one, we dont need to fit the intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f44c75",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now all we have to do is to run the `.fit()` method of the `LinearRegression` object, using our `X` and `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fe975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c6861",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `coef` attribute of the `LinearRegression` model object. Plot the values. What do they correspond to?\n",
    "\n",
    "Hint: the coefficients correspond to the features in the columns of our X matrix! Which in our case are image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847822d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027c7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a869201",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now let's plot the model coefficients overlaid with the image tuning curve for this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "\n",
    "# plot the coefficients on one axis\n",
    "ax.plot(model.coef_, 'o', label='model coeff.')\n",
    "ax.set_xticks(np.arange(0, len(image_names)))\n",
    "ax.set_title('cell index: '+str(cell_index))\n",
    "\n",
    "# plot the cell responses on another\n",
    "ax = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_names, label='dF/F', linestyle='None', ax=ax, zorder=0)\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c441de",
   "metadata": {},
   "source": [
    "Looks pretty good right?? \n",
    "\n",
    "Actually, this is ***very very wrong***. \n",
    "\n",
    "We just fit our model with our full dataset, of course it gave us the exact right answer. \n",
    "\n",
    "And how would we know if it can predict anything accurately if we have no data to test it with?!\n",
    "\n",
    "The **right** thing to do here, is to split up our data into training and test sets, then to train the model on the training set, and test the model on the test set. Then we can evaluate how effective our model is at predicting unobserved data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a43e0f",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f9065",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Regression provides a *Predictive* model. For new values of `X`, we can produce an estimate of what `y` should be. If we just provide all the values of `X` and `y` that we have to the model, as we did above, we cant test whether it predicts anything. \n",
    "\n",
    "Importantly, the predictive nature of our model also proves to be an important tool for assessing whether our model consistently represents our data. \n",
    "\n",
    "We do this by splitting our dataset into parts. We will train the model on on part of our dataset, which we call the **training set**, then evaluate it on data that was withheld from this initial training, which we call the **test set**.\n",
    "\n",
    "`sklearn` has a useful method to split the data into training and test sets, a function called `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(4)# Setting the random seed here insures that everyone gets the same result when they run this notebook!\n",
    "\n",
    "# Use sklearn train_test_split \n",
    "y_train, y_test, X_train, X_test = train_test_split(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eeef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do each of these splits look like? \n",
    "print('length of y_train', len(y_train))\n",
    "print('length of y_test', len(y_test))\n",
    "print('length of X_train', len(X_train))\n",
    "print('length of X_test', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754bfef",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now fit the model with the training set. The **right** way.\n",
    "\n",
    "Plot the model coefficients overlaid on the original data, as we did before. Does it look different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b175135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20583b4e",
   "metadata": {},
   "source": [
    "Ok thats more like it. It's not perfect. Its a prediction. \n",
    "\n",
    "Now we can evaluate performance of the model using the test data that we held out from fitting the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b605f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Let's look at how models are evaluated.\n",
    "\n",
    "The `LinearRegression` object has a method to evaluate the `score` of the model. \n",
    "    \n",
    "The default score used in `sklearn` is called the coefficient of determination, or $R^2$. This number gives the fraction of the variance in in a dataset that is described by the model. Specifically, it compares the ratio of the sum of squared residuals for the model:\n",
    "    \n",
    "   $$SS_{residuals} = \\sum_i{(y_i-f_i)^2}$$\n",
    "    \n",
    "And compares this the sum of squared residuals if you had just used the mean of your data, $\\bar{y}$, as your model (this will be proportional to the model variance). This later quanity is called the \"total sum of squares\"\n",
    "    \n",
    "   $$SS_{total} = \\sum_i{(y_i-\\bar{y})^2}$$\n",
    "$R^2$ is defined:\n",
    "    \n",
    " $$R^2 = 1-\\frac{SS_{residuals}}{SS_{total}}$$\n",
    "    \n",
    "\n",
    "$R^2$ will be 1 for a model that captures 100% of the avalible variance, and 0 for a model that captures non. If can also be less than 0 if using your model provides a worse prediction if you had just guessed with the mean of your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f79196",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `score` method of the LinearRegression object to evaluate the model's performance on our held-out testing data (`X_test` and `y_test`)\n",
    "\n",
    "Repeate for the training data. \n",
    "\n",
    "Is the score on the test data lower or higher than the score for the training data? Is it greater than or less than zero? What can we learn from answering these questions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8b971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a14b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02237d9",
   "metadata": {},
   "source": [
    "Hmm... what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6827b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "As stated above: \n",
    "\n",
    "$R^2$ will be 1 for a model that captures 100% of the avalible variance, and 0 for a model that captures non. If can also be less than 0 if using your model provides a worse prediction if you had just guessed with the mean of your data!\n",
    "\n",
    "So... this model is not doing very well. Maybe this cell just isnt well explained by images. \n",
    "\n",
    "Let's try our other cell, cell index 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29331034",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "#### Create a feature matrix, split the data, fit the model, and evaluate the results for cell 6\n",
    "\n",
    "1. Build feature matrix `X` (images across trials) and predictor `y` (cell repsonses across trials)\n",
    "2. Split the data into training and test sets \n",
    "3. Fit the model using the training data \n",
    "4. Evalute model accuracy on training and test sets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ada853",
   "metadata": {},
   "source": [
    "Let's do all the steps at once below, for our cell that appeared to have image tuning in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stimulus responses for our cell\n",
    "cell_index = 6\n",
    "cell_specimen_id = ophys_experiment.cell_specimen_table.index.values[cell_index]\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "\n",
    "# Get the image presentations for all the change trials in our stim response df\n",
    "image_presentations = cell_df.image_name.values\n",
    "\n",
    "# initialize an empty matrix that is the length of the number of image presentations by 8, the number of unique images\n",
    "X = np.zeros((len(image_presentations), 8))\n",
    "\n",
    "image_names = np.unique(image_presentations)\n",
    "# Loop through image presentations and set the column corresponding to that image index to 1 for each row\n",
    "for i, image_name in enumerate(image_presentations):\n",
    "    image_index = np.where(image_names==image_name)[0][0]\n",
    "    X[i, image_index] = 1\n",
    "\n",
    "# y is just our image responses\n",
    "y = cell_df.mean_response.values\n",
    "\n",
    "# Split the data\n",
    "np.random.seed(4)# Setting the random seed here insures that everyone gets the same result when they run this notebook!\n",
    "\n",
    "# Use sklearn train_test_split \n",
    "y_train, y_test, X_train, X_test = train_test_split(y, X)\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the score on the train and test sets\n",
    "\n",
    "# Check the model score using the test data\n",
    "print('Score for held out test data:', model.score(X_test, y_test))\n",
    "\n",
    "# Check the model score using the training data\n",
    "print('Score for training data:', model.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb39670",
   "metadata": {},
   "source": [
    "Ok, this is a bit better. Nothing is negative this time. \n",
    "\n",
    "A score of 0.18 for the held out test data indicates that we can explain 18% of the variance in our test data using this model. Thats decent. But, it is much less than the score for the training data, which is 0.37, meaning that we arent doing as well at predicting the held out test data as we are on the data used to train the model to begin with. \n",
    "\n",
    "Still, the score on the test data is greater than 0, which means our model has SOME predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb8f53",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4> How do you ensure that your model is valid and is not \"overfitting\"?</h4>\n",
    "<p>\n",
    "\n",
    "\"Overfitting\" is a term used to describe the case in which learns does very well in describing the data that it is trained on, but fails to predict new or additional data. Another way of saying this that a model will learn to describe noise or idiosyncrasies of the training data, rather than the underlying relationships that you are trying to model.\n",
    "\n",
    "To illustrate overfitting, lets pause for a quick thought experiment. Imagine that, instead of fitting the two parameter model we just used, we fit a model with $N$ parameters where $N$ is the number of data points. Our model, which could look like this:\n",
    "\n",
    "$𝑓(𝑥_i)=\\sum_{i = [1,N]} w_i\\vec{x}_i$\n",
    "\n",
    "We call this a \"saturated model\" because it is saturated with parameters. Once we fit this model, we would discover that we could now *perfectly* predict every single data point. In this linear case, we would now find $f(x_i)=y_i$, with error of 0. \n",
    "\n",
    "So...why don't we do this? Wasn't our goal to get the lowest error possible? \n",
    "\n",
    "You have probably already noticed the two big problems with this saturated model. First, we can't learn anything from its weights. Regression is a guided dimensionality reduction exercise, where we try and describe our data with a chosen set of features. The saturated model fails to do this. Second, this model is worthless for explaining new data. It assumes a 1-to-1 $x_i\\rightarrow y_i$ mapping, and is undefined for new points or allows assumes no variance for repeated observations.\n",
    "\n",
    "Even if we move away from this extreme case, it is still possible to overfit a model simply by fitting weights to too many features. Spurious correlations in your data will appear to be explained by the additional features in your training data, only to limit your ability to predict held out data. \n",
    "\n",
    "<b>\n",
    "In the next section, we will introduce a technique known as \"Cross-Validation\" as a way to systematically test if you have a \"good\" model.\n",
    "\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09e138",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29151",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "This \"training\" and \"testing\" split approach would be great if data were always cheap and plentiful. In practice however, it can be frustrating to use most of your hard-earned data to train a model, only learn how it performs on a held out subset.\n",
    "\n",
    "A common aproach to dealing with this problem is known as <b>*Cross Validation*</b> Here, we systematically hold out chunks of data, refitting our model on the remaining data each time. By performing multiple model fits, we can (1) use all our data and (2) get a better sense of how our data varies across the dataset.\n",
    "\n",
    "There are many ways to do cross validation, and how you split things up can have a big influence on the question you are trying to answer. Lets start with one of the simplist and most common forms of cross validation, known as <b>KFolds</b>. Here, we split (i.e. fold) our data $k$ times, with equal sample sizes in each fold. We then fit $k$ models to our data. This way you can use all your data, and test model performance multiple times, with each fold serving as its own train-test split.\n",
    "\n",
    "What is <b>$k$</b>? We will use 5 for now. It can be anywere from 2 to $n$ where $n$ is the number of samples in your dataset. This extreme case, where $n$ is the same as the number of samples you have is called \"leave-one-out\" cross validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addeb0e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Similar to `test_train_split`, `sklearn` provides a convient object for splitting data for cross-validation, so that you don't need to write your own splitting code. It is called `KFolds` and is housed in the `model_selection` modual.\n",
    "\n",
    "Just as we did with the `Regression` object, the `scikit-learn` interface has us instantiate a `KFold` object, which provides a generator object that we can use to loop through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generate the folds\n",
    "folderizer = KFold(n_splits=5)\n",
    "folderizer.split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83774ae7",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "What is a generator object? Each time it is called, it will generate the next of n folds in our data. It can therefore be incorporated into a for loop like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the seed so you get the same result here no matter what order you run this notebook in!\n",
    "np.random.seed(9) \n",
    "\n",
    "# Initialize KFold object\n",
    "n_folds = 5\n",
    "folderizer = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Create arrays to save the results\n",
    "self_score = np.empty(n_folds)\n",
    "cross_score = np.empty(n_folds)\n",
    "models = [None]*5\n",
    "\n",
    "# Loop over folds, fit the model and collect the scores\n",
    "for ii, (train_index, test_index) in enumerate(folderizer.split(X, y)):\n",
    "    models[ii] = LinearRegression(fit_intercept=False).fit(X[train_index,:], y[train_index])\n",
    "    self_score[ii] = models[ii].score(X[train_index,:], y[train_index])\n",
    "    cross_score[ii] = models[ii].score(X[test_index,:], y[test_index])\n",
    "print(f'Training Score: {self_score}')\n",
    "print(f'Testing Score: {cross_score}')\n",
    "print(f'Average Testing Score: {np.mean(cross_score)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc3b0e",
   "metadata": {},
   "source": [
    "Discussion questions:\n",
    "\n",
    "(1) Why is this different from what we did before with `train_test_split`? \n",
    "\n",
    "(2) What does this tell us about image encoding in our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a05c09",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Here is what is happening each time **KFolds** generator object is called (iterations of the generator are on the y axis). \n",
    "\n",
    "The top plot shows the default behavior - it splits the data into 5 even sequential chunks. \n",
    "\n",
    "Since we said shuffle = True when we created the KFolds object (`KFold(n_splits=n_folds, shuffle=True)`), the splits were not sequential, the folds were created by randomly sampling across trials.\n",
    "This is a good thing to do if there may be an effect of order in your data points (in our case, trials are in order of time in the session, which could correlate with task engagement or other behavioral changes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7fbc8",
   "metadata": {},
   "source": [
    "![sklearn_kfold.png](../resources/sklearn_kfold.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68a22c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Note that neither of these approaches take into account the class or group structure of the data. \n",
    "For example, if we had used all image presentations rather than only the image changes as the input to our model,\n",
    "the stimulus presentations would have been grouped into repeated presentations of the same stimulus in between the changes. \n",
    "We could also consider periods where the mouse is running vs. not running as different groups that you may want to consider separately.\n",
    "\n",
    "You can learn more about options for KFold generator objects here: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7e3c6",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Loop through the folds, as we did above, but now plot the coefficients for each fold to check how consistent the results are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecf501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f97edd24",
   "metadata": {},
   "source": [
    "Are the model coefficients consistent across folds? \n",
    "\n",
    "What about other features that could influence the cell's activity? Is there an effect of behavioral choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54961f",
   "metadata": {},
   "source": [
    "<div style=\"background: #E6E6FA; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "<h3> Multiple linear regression: Stimulus & choice encoding </h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7875c67",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Does the prediction improve when additional variables are included? </h4>\n",
    "\n",
    "\n",
    "<h4> Which features best explain a cell's activity? </h4>\n",
    "\n",
    "*Multiple Linear Regression* gives us tools to dissect the contributions of different features in explaining variance. By fitting models with different combinations of features, we can get a richer sense of how different features are encoded by neural activity. \n",
    "\n",
    "Just as we built a design matrix out of different stimulus identities, we can similary construct one that includes additional features about our data, such as whether a trial was a hit or miss, or what the running speed of the mouse was on a given trial. \n",
    "\n",
    "We can then ask how much each feature contributes to explaning the cell's variance by comparing to models with one or more of the features left out. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac963bb",
   "metadata": {},
   "source": [
    "\n",
    "This next step is going to involve a bunch of model fitting, using the same basic procedure we outlined in the previous section. Before we go on, lets take a quick momement to move our KFold Linear Model fitting into a function so we don't have to type so much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidateLinearModel(X, y, n_split = 5, shuffle = False, shuffle_seed = None):\n",
    "    '''\n",
    "    Cross validate a linear model using KFold cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input data to fit\n",
    "    y : np.array\n",
    "        The output data to fit\n",
    "    n_split : int\n",
    "        The number of splits to use\n",
    "    shuffle : bool\n",
    "        Whether or not to shuffle the data\n",
    "    shuffle_seed : int\n",
    "        The seed to use for shuffling the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Mean Score: float\n",
    "        The average cross validation score\n",
    "    Model List: list    \n",
    "        The models fit to each fold of the data\n",
    "    Test score: np.array    \n",
    "        The cross validation scores for testing data each fold\n",
    "    Train score: np.array\n",
    "        The cross validation scores for testing data each fold\n",
    "    '''\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "        X = X.copy().reshape(-1,1)\n",
    "    # Initialize KFold object\n",
    "    folderizer = KFold(n_splits=n_split, shuffle=shuffle, random_state=shuffle_seed)\n",
    "    # Create an array to save the results\n",
    "    self_score = np.empty(n_folds)\n",
    "    cross_score = np.empty(n_folds)\n",
    "    models = [None]*n_split\n",
    "    # Loop through the folds, fit the model, and save the results\n",
    "    for ii, (train_index, test_index) in enumerate(folderizer.split(X, y)):\n",
    "        models[ii] = LinearRegression(fit_intercept=False).fit(X[train_index,:], y[train_index])\n",
    "        self_score[ii] = models[ii].score(X[train_index,:], y[train_index])\n",
    "        cross_score[ii] = models[ii].score(X[test_index,:], y[test_index])\n",
    "        \n",
    "    return np.mean(cross_score), models, cross_score, self_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282405fb",
   "metadata": {},
   "source": [
    "Note that the `KFold` object includes both `shuffle` and `shuffle_seed` parameters. `shuffle` does exactly what it sounds like- it randomizes the set data points included in each fold. `shuffle_seed` can be used to get reproducible results from this shuffling. This is particularly important if we want to compare models- using the same shuffle seed will give the same random set of trials across function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f9520",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Impact of behavioral choice on model prediction </h4>\n",
    "\n",
    "Above we saw that there may be a difference in cell activity between `hit` and `miss` trials (image changes with or without a lick). \n",
    "\n",
    "Let's build a model that includes whether a trial was a hit or miss and see whether the prediction improves. \n",
    "\n",
    "To do this, we will add information about whether each trial was a hit or a miss to our `X` matrix using the same one-hot encoding method we used for the stimulus, then use this to quanitfy the relative contribution of stimulus identity vs. choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f462a62",
   "metadata": {},
   "source": [
    "Let's get our y-values again - the thing we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y values for cell responses\n",
    "y_values = cell_df.mean_response.values\n",
    "# Look at the first 10 values\n",
    "y_values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20653f69",
   "metadata": {},
   "source": [
    "Here we will create an X matrix that includes whether each trial was a hit or a miss, in addition to the stimulus identity, by adding a new column with a boolean value for hits and misses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoded vector indicating whether each trial is a hit or a miss\n",
    "# Which is essentially just taking the Boolean \"hit\" column from our original cell dataframe and converting it to zeros and ones\n",
    "\n",
    "# Get boolean values for hit vs miss\n",
    "hits_bool = cell_df.hit.values\n",
    "hits_bool = hits_bool != 0 # This makes sure its a boolean array not a python object\n",
    "hits_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be0043",
   "metadata": {},
   "source": [
    "Turn it into zeroes and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of zeros to represent the misses\n",
    "X_hit = np.zeros(hits_bool.shape)\n",
    "# Set the trials where hit = True to 1\n",
    "X_hit[hits_bool==True] = 1\n",
    "X_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898282",
   "metadata": {},
   "source": [
    "Now combine our original `X` matrix, which contains the 8 image identities, with our hits to create a new design matrix with stimuli and hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First recall that the original X matrix is the length of trials by the number of images\n",
    "# To merge the hits matrix, which is 1D with the stimulus X matrix, which is 2D, we need to reshape X_hits\n",
    "X_hit = X_hit.reshape(-1,1)\n",
    "print(X_hit.shape, 'shape of matrix just for hits')\n",
    "\n",
    "# Now we can join them into a single X matrix, with 8 columns for images and 1 column for hits\n",
    "X_stim_choice = np.hstack((X, X_hit))\n",
    "print(X_stim_choice.shape, 'shape of matrix after adding hits to stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0a6a0",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the X matrix to check what it looks like. Zoom in on the first 20 trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf399fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27bba97",
   "metadata": {},
   "source": [
    "There is an additional trick here, which you may have learned about if you had read about the `trials` table (which we previously used to annotated our `cell_df`) in the **DataBook**\n",
    "\n",
    "The first 5 trials of each behavior session are `autorewarded`, meaning that the mouse is automatically given water when the stimulus changes. \n",
    "\n",
    "We want to remove these from our design matrix, because they are not real choices. \n",
    "\n",
    "You can read about the `trials` table here: https://allenswdb.github.io/physiology/ophys/visual-behavior/VB-BehaviorSessionData.html#trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705d67a",
   "metadata": {},
   "source": [
    "Let's remove the first 5 trials, which are free rewards, not actual choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y values for cell responses and remove first 5 trials\n",
    "y_values = cell_df.mean_response.values\n",
    "y = y_values[5:]\n",
    "\n",
    "# Make X matrix for images and hits with first 5 trials removed\n",
    "# Stimuli only\n",
    "X_stim = X[5:]\n",
    "# Hits only \n",
    "is_hit = cell_df.hit.values[5:]\n",
    "is_hit = is_hit != 0 # This makes sure its a boolean array not a python object\n",
    "X_hit = np.zeros(is_hit.shape)\n",
    "X_hit[is_hit==True] = 1\n",
    "X_hit = X_hit.reshape(-1,1) # Reshape so it can be merged with stimuli\n",
    "# Combined stimulus and choice\n",
    "X_stim_choice = np.hstack((X_stim, X_hit))\n",
    "\n",
    "# Print the shapes to be sure they match\n",
    "print(y.shape)\n",
    "print(X_stim.shape)\n",
    "print(X_hit.shape)\n",
    "print(X_stim_choice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe5f8b",
   "metadata": {},
   "source": [
    "Ok good, all of our matrices have shrunk by 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it to check\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(X_stim_choice[:20])\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Trial #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325ea3f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now use the `crossValidateLinearModel` function to fit and test performance of a model including hits in the design matrix, `X_stim_choice`. Use `shuffle_seed`=5. \n",
    "\n",
    "How well does it fit the data? \n",
    "\n",
    "Run the function again using your original X matrix that only included stimulus identity, `X_stim`. \n",
    "\n",
    "How does model performance compare with the model that included hits and misses? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc175a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9aaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dadde7bb",
   "metadata": {},
   "source": [
    "Are they different? What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc3cf8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "This result may be surprising, given that previously we showed that tuning can be different on hits vs misses.\n",
    "\n",
    "To understand why this is happening, we need to think about what question we are asking with our model. In the model we just created, we used a single feature to encode whether a trial was a hit or a miss. This allows us to assess whether the overall activity is different on hits vs misses, and whether that accounts for more variance than stimulus information, but what if there are more complex differences in tuning as a function of choice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c059f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remind ourselves what our cell tuning looks like\n",
    "\n",
    "# Plot the mean response for each image split by hit and miss\n",
    "\n",
    "image_order = np.sort(cell_df.image_name.unique())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_order, hue='hit', linestyle='None', ax=ax)\n",
    "ax.set_xticklabels(image_order, rotation=90)\n",
    "ax.set_title('cell index: '+str(cell_index))\n",
    "ax.set_ylabel('dF/F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103ef0a",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Relationships between features </h4>\n",
    "\n",
    "Looks like the cell encodes a combination of stimulus information and choice information (i.e. the change in response due to hits is not generic across all images, it is image specific). But our X matrix only considers the identity of each image or whether the trial is a hit, not the combination of image identity and hits. \n",
    "\n",
    "There are a couple ways to address this situation. One would be to adapt the linear modeling framework to handle two conditions of stimuli: hits vs misses. That is, each of the 8 images during hits, and each of the 8 images during misses. \n",
    "\n",
    "For this we will build two copies of the stimulus X matrix, each with a column for whether the trial was a hit or a misss, except one will be true for hits and one will be true for misses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build two copies of the stimulus matrix: one for hits and one for misses\n",
    "# When a trial is not in that category, set all values to 0.\n",
    "X_hit_stim = X_stim.copy()\n",
    "X_hit_stim[~is_hit, :] = 0\n",
    "\n",
    "# You can use np.invert() to flip the boolean array\n",
    "X_miss_stim = X_stim.copy()\n",
    "X_miss_stim[is_hit, :] = 0\n",
    "\n",
    "# Combine everything into a single design matrix.\n",
    "X_stim_per_choice = np.hstack((X_hit_stim, X_miss_stim))\n",
    "\n",
    "# Check the shape - how mamy columns are there now?\n",
    "print(X_stim_per_choice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d6a6e",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the design matrix for `X_stim_per_choice`.\n",
    "\n",
    "How is it different than our previous design matrix `X_stim_choice`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13357b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01619a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7013d2",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Let's use the `crossValidateLinearModel` function again to predict activity for our new X matrix (`X_stim_per_choice`) with each image represented separately for hits and misses. \n",
    "\n",
    "We will make sure to collect all the outputs of this function so that we can assess the variability and model fits (Hint: The function returns: `average_test_score`, `models`, `test_score`, `train_score`). \n",
    "\n",
    "What is the average score? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all the outputs of the function so we can look at how it did across splits\n",
    "n_split = 5\n",
    "X_stim_per_choice_score, models, test_score, train_score = crossValidateLinearModel(X_stim_per_choice, y, \n",
    "                                                                n_split=n_split, shuffle=True, shuffle_seed=5)\n",
    "print('average_score:', X_stim_per_choice_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241ba44",
   "metadata": {},
   "source": [
    "Is it better than before? How do we know whether to trust the average score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b0d51",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Evaluate the model outputs (the `test_score` and `train_score` across folds) for the model fit with `X_stim_per_choice`.\n",
    "\n",
    "Do you get a different result compared to when the hits and images were represented separately (with `X_stim_choice`)?\n",
    "\n",
    "How consistent are the scores across folds? \n",
    "\n",
    "Is the model over fitting? How does the score for the training data compare to the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb2c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8450b1b",
   "metadata": {},
   "source": [
    "Are the scores consistent across folds? If not, what could explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33b805",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Let's plot the model coefficients across each fold and compare to the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a plot axis to visualize the results\n",
    "fig,ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# Loop over folds and plot the coefficients\n",
    "for i in range(n_split):\n",
    "    ax[0].plot(models[i].coef_[:8], color='b', label='hits', marker='o')\n",
    "    ax[0].plot(models[i].coef_[8:], color='r', label='misses', marker='o')\n",
    "# Label the axes\n",
    "ax[0].set_xlabel('Image name')\n",
    "ax[0].set_ylabel('Model Coefficient')\n",
    "ax[0].set_xticks(np.arange(0,8))\n",
    "ax[0].set_xticklabels(image_names, rotation = 90);\n",
    "ax[0].legend(['hits', 'misses'], fontsize='xx-small')\n",
    "\n",
    "# Now plot the data\n",
    "palette = ['r', 'b']\n",
    "ax[1] = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_order, hue='hit', \n",
    "                      hue_order=np.sort(cell_df.hit.unique()), palette=palette, ax=ax[1])\n",
    "ax[1].set_xticklabels(image_order, rotation=90)\n",
    "ax[1].set_title('cell index: '+str(cell_index))\n",
    "ax[1].set_ylabel('dF/F')\n",
    "ax[1].legend(fontsize='xx-small', title='hit', title_fontsize='xx-small')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5db906",
   "metadata": {},
   "source": [
    "Looks pretty good actually...\n",
    "\n",
    "But still, looking at the test and train scores across folds suggests that something may be sub-optimal, as the score across folds is pretty variable, and the train scores are consistently much higher than the test scores which suggests overfitting.\n",
    "\n",
    "What could explain this? What else do we need to take into account?\n",
    "\n",
    "Could there be a bias in when the hits and misses occur within the session? This could influene variability in the model fits across folds, depending on exactly which trials are randomly selected for the fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c8614",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "If the categories in our X matrix arent balanced, we could end up with a sub-optimal, unreliable model\n",
    "\n",
    "Let's check the number of hits and misses to see whether they are balanced, and evaluate when they occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9828e",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Print out the number of trials that are hits and the number that are misses. \n",
    "\n",
    "Plot your `is_hit` boolean array to see when in the session the hits and misses occur. \n",
    "The x axis will be trial # and the y axis will be whether it was a hit or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d618be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399536fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5218c0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Dealing with uneven samples </h4>\n",
    "\n",
    "Another factor to consider when setting up a model is the sampling within our groups (hit and miss). \n",
    "\n",
    "You will notice that there are way more miss trials than hit trials, and that they are not evenly distributed through the session. If we were to just grab trials without paying attention to this, we could undersampling the hits and as a result overfitting our model. \n",
    "\n",
    "Fortunately, `sklearn` includes tools to do **\"stratification\"** with its data splitting & cross-validation approaches. Stratification means that the folds will preserve the percentage of samples in each class. \n",
    "\n",
    "The `train_test_split` function has a 'stratify' input, which takes an Boolean array corresponding to your distinct classes that you want to sample within. \n",
    "\n",
    "For `KFolds`, there is a distinct generator object for stratified folds, called `StratifiedKFold`, which takes an argument called `group` when you generate the split. `StratifiedKFold` only works for multi-class or binary variables for `y`, which is not what we have here since our `y` variable is an array of continuous values of cell activity, but its still useful to look at how it works to understand stratification.\n",
    "\n",
    "The `StratifiedKFold` generator works like this: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb1bd8",
   "metadata": {},
   "source": [
    "![sklearn_stratifiedKfold.png](../resources/sklearn_stratifiedKfold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99a803",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use `train_test_split` to generate training and test sets with our new X matrix that contains images separately for hits and misses (`X_stim_choice`)\n",
    "\n",
    "Provide the `is_hit` Boolean array that we created above as the input to the `stratify` argument. \n",
    "\n",
    "Then fit a model with the `LinearRegression` object as we did before to evaluate model performance (by checking the `score`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb46036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data split using stratify input to preserve percentages of each class (hit and miss)\n",
    "y_train, y_test, X_stim_choice_train, X_stim_choice_test = train_test_split(y, X_stim_choice, stratify=is_hit, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf38c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LinearRegression object with X_separate_stim matrix and the y value (cell responses) to predict\n",
    "lr = LinearRegression(fit_intercept=False).fit(X_stim_choice_train, y_train)\n",
    "lr.score(X_stim_choice_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132ee0a",
   "metadata": {},
   "source": [
    "Ok now let's do it in a loop with 5 folds, and see how consistent the scores are across folds when we use the stratification approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a32326",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "# Create an empty array to save the data in\n",
    "strat_train_score = np.empty(n_folds)\n",
    "strat_test_score = np.empty(n_folds)\n",
    "# Loop over folds, fit the model, and compute the scores on training & test data\n",
    "for ii in range(n_folds):\n",
    "    y_train, y_test, X_stim_choice_train, X_stim_choice_test = train_test_split(y, X_stim_choice, stratify=is_hit, shuffle=True)\n",
    "    lr = LinearRegression(fit_intercept=False).fit(X_stim_choice_train, y_train)\n",
    "    strat_train_score[ii] = lr.score(X_stim_choice_train, y_train)\n",
    "    strat_test_score[ii] = lr.score(X_stim_choice_test, y_test)\n",
    "# Print out the results\n",
    "print(f'Training scores: {strat_train_score}')\n",
    "print(f'Testing scores: {strat_test_score}')\n",
    "\n",
    "# Check the mean and variance of test scores\n",
    "print('Mean of test scores:', np.mean(strat_test_score))\n",
    "print('Variance of test scores:', np.var(strat_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original version, before stratification\n",
    "# Check the score for all folds\n",
    "print('train scores:', train_score)\n",
    "\n",
    "# Check the score for all folds\n",
    "print('test scores:', test_score)\n",
    "\n",
    "# Check the mean and variance of test scores\n",
    "print('mean of test scores:', np.mean(test_score))\n",
    "print('variance of test scores:', np.var(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4df34",
   "metadata": {},
   "source": [
    "Is the score higher for the stratefied approach compared to our previous method? \n",
    "\n",
    "Are the scores more or less variable? \n",
    "\n",
    "Are you convinced that balancing trial numbers for unevenly sampled groups matters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5d9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3589aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd881e8c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h2> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc2b22",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p> \n",
    "Regression is a powerful way to quantitativly probe the relationships between variables. In this tutorial, we have seen how it can be used to understand what and how various stimuli and behavioral features are encoded by single neurons.\n",
    "\n",
    "<p>  \n",
    "In the interest of time, we will end by admitting we the most certainly haven't shown you everything that you can do with regression. We will close this tutorial quickly highlighting a few other useful regression applications that you might find yourself wanting later in the course (or beyond!).\n",
    "\n",
    "<p> \n",
    "  <b>  (1) Regression is a great way to survey a large amount of data.</b>  \n",
    "    <p>\n",
    "        As datasets get biger and easier to collect, we need tools that allow us to probe many cells or variables in an efficient manor. Regression models offer a way to screen large amounts of data. You might, for example, fit a regression model to every cell in the Visual Behavior dataset to ask \"which cell types are most modulated by running speed.\" \n",
    "<p>\n",
    "In the next tutorial, we will learn about a related set of tools called \"Decoding Models.\" These models allow you understand what is encoded in a neural population by explicity decoding that information from a recoding. These techniques are often used in tandem - you might, say, decode running from an entire population of neurons, then use regression models to look at how individual cells suport your decoding analysis.\n",
    "    \n",
    "<p>   \n",
    "<b> (2) Regression models are predictive </b>\n",
    "<p>\n",
    "Here, we focused entirly on cases where we were predicting similar but held out data with our model. Often, however, we use regression to look at differences between condition by fitting a model on one condition and predicting on the other. This is very related to what we did with hits and misses: by seperating the design matrix there, we effectivly trained one model on 'hits' and another on 'misses.' An alternative way of doing this analysis would have been to see how well (or badly) the hit model did at predicting miss trials or visa-vera.\n",
    "    \n",
    "<p>\n",
    "<b> (3) Linear models are the simplist,  but there is no rule that regression needs to be linear</b>\n",
    "<p>  \n",
    "Everything we have done here is commonly refered to as a \"General Linear Model,\" or gLM. These are super useful, but this model class makes a big assumption about the variance distribution of our data: specifically, we assumed that variance is normally distributed about some mean value, $f(x)$. This is mathmatically the easiest thing to do, but it turns out to not always be the best assumption.\n",
    "\n",
    "    \n",
    "<p> \n",
    "In fact, there is a very specific case that often arrises in neuroscience where we know this assumption to be particularly bad: Spike trains. Spikes are discrete, transient events. Over a given time interval, there can never be fewer than 0 spikes, and there can never be a fractional number of spikes. If we assume that spikes are occure indepently with an average rate (call it $\\lambda$), then a poisson distribution (https://en.wikipedia.org/wiki/Poisson_distribution) can be shown to be a good model of this variance. We can make a regression model that assumes poisson variance if we fit the following:\n",
    "    \n",
    "<p>\n",
    "$ln(\\lambda) = \\beta X$ or equivalently $\\lambda = e^{\\beta X}$\n",
    "    \n",
    "<p>\n",
    "Here, adding the $ln(*)$ term makes the model harder to fit, but fortunatly, `sklearn` ships with an implementation of poisson regression. It can also handle other commonly used non-linearities (called link functions) (e.g. those for exponential data, logistic data, etc. https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models). \n",
    "\n",
    "    \n",
    "<p>\n",
    "Because this class of models is a generalization of the general linear model, they are commonly called \"General*ized* Linear Models,\" or GLMs. Yes, this nomenclature is confusing- if it makes you feel any better, the people who created it later admitted they should have chosen a better name. You will sometimes also hear of these models refered to as \"Linear-Non-linear\" models, because the linear and nonlinear parts can be mathematically seperated.\n",
    "\n",
    "<p>\n",
    "All of the tools we have discussed for linear regression can be applied to GLMs. More information on this method is avalible in the Data Book!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0efb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc23002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c8485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.770791,
   "end_time": "2023-07-31T19:18:26.861555",
   "environment_variables": {},
   "exception": null,
   "input_path": "doc_template/examples_root/examples/nb/visual_behavior_load_ophys_data.ipynb",
   "output_path": "/tmp/tmpcq7t4kmr/scratch_nb.ipynb",
   "parameters": {
    "output_dir": "/tmp/tmpcq7t4kmr",
    "resources_dir": "/home/runner/work/AllenSDK/AllenSDK/allensdk/internal/notebooks/resources"
   },
   "start_time": "2023-07-31T19:17:20.090764",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
